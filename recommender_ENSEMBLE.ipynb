{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq8fYR6NY_7_"
      },
      "source": [
        "# init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmUgYPsj0Tw-",
        "outputId": "ae3e58b5-9821-4a9b-d5d9-a9d6191f532b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io.arff import loadarff\n",
        "from pymfe.mfe import MFE\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn import svm\n",
        "import sklearn.metrics as skm\n",
        "from sklearn import metrics\n",
        "import pdb\n",
        "import random\n",
        "import xgboost as xgb\n",
        "from catboost import Pool, CatBoostRegressor\n",
        "from scipy.stats import friedmanchisquare\n",
        "from scikit_posthocs import posthoc_ttest\n",
        "\n",
        "stdTable = \"./data/recommender/std-table.csv\"\n",
        "path_arff = \"./data/datasets/arff/\"\n",
        "path_index = \"./data/index/arff/\"\n",
        "# files_arff = os.listdir(path_arff)\n",
        "\n",
        "path = \"./data/experimento/datasets/selected/cleaned/\"\n",
        "tablePath = savePath = \"./data/recommender/meta-features.csv\"\n",
        "\n",
        "ccTable = \"./metafeatures/meta-table-CC.csv\"\n",
        "accTable = \"./metafeatures/meta-table-ACC.csv\"\n",
        "smmTable = \"./metafeatures/meta-table-SMM.csv\"\n",
        "hdyTable = \"./metafeatures/meta-table-HDy.csv\"\n",
        "dysTable = \"./metafeatures/meta-table-DyS.csv\"\n",
        "sordTable = \"./metafeatures/meta-table-SORD.csv\"\n",
        "msTable = \"./metafeatures/meta-table-MS.csv\"\n",
        "# ms2Table = \"./metafeatures/meta-table-MS2.csv\"\n",
        "maxTable = \"./metafeatures/meta-table-MAX.csv\"\n",
        "xTable = \"./metafeatures/meta-table-X.csv\"\n",
        "t50Table = \"./metafeatures/meta-table-T50.csv\"\n",
        "\n",
        "algList = [\"CC\",\"ACC\",\"SMM\",\"HDy\",\"DyS\",\"SORD\",\"MS\",\"MAX\",\"X\",\"T50\"]\n",
        "pathList = [ccTable, accTable, smmTable, hdyTable, dysTable, sordTable, msTable, maxTable, xTable, t50Table]\n",
        "\n",
        "experiment_tables_path = './experiment_tables/'\n",
        "experiment_tables_dict = None\n",
        "processed_datasets_df = None\n",
        "\n",
        "datasets_path = './datasets/'\n",
        "files = os.listdir(datasets_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHUI9P42ZC5o"
      },
      "source": [
        "# preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_experiment_tables():\n",
        "    exp_tables_dict = {key: None for key in ['CC', 'ACC', 'PACC', 'PCC', 'SMM', 'HDy', 'DyS', 'SORD', 'MS', 'MS2', 'MAX', 'X']}\n",
        "        \n",
        "    for key in exp_tables_dict.keys():\n",
        "        if os.path.isfile(experiment_tables_path + 'experiment_table_' + key + '.csv'):\n",
        "            exp_tables_dict[key] = pd.read_csv(experiment_tables_path + 'experiment_table_' + key + '.csv')\n",
        "        else:\n",
        "            exp_tables_dict[key] = pd.DataFrame(columns=['dataset_name', 'alpha', 'sample_size', 'real_p', 'pred_p', 'abs_error', 'run_time'])\n",
        "    \n",
        "    return exp_tables_dict\n",
        "\n",
        "experiment_tables_dict = load_experiment_tables()\n",
        "df_dict = {key: None for key in list(experiment_tables_dict.keys())}\n",
        "\n",
        "\n",
        "for key in experiment_tables_dict:\n",
        "    df_dict[key] = experiment_tables_dict[key].groupby('dataset_name')['abs_error'].aggregate('mean')\n",
        "\n",
        "meta_features_table = pd.read_csv('./metafeatures/meta-features-table.csv')\n",
        "\n",
        "algList = []\n",
        "tableList = []\n",
        "for counter in df_dict.keys():\n",
        "    algList.append(counter)\n",
        "\n",
        "    y = df_dict[counter].values\n",
        "\n",
        "    X = meta_features_table.values\n",
        "    np.nan_to_num(X, copy=False)\n",
        "    \n",
        "    row, column = np.where(X > np.finfo(np.float32).max)\n",
        "    for i in range(len(row)):\n",
        "        X[row[i]][column[i]] = np.finfo(np.float32).max\n",
        "\n",
        "    tableList.append((X, y))\n",
        "\n",
        "processed_datasets_df = pd.read_csv('./experiment_tables/processed_datasets.csv')\n",
        "datasets = processed_datasets_df['dataset'].tolist()\n",
        "\n",
        "for key in df_dict.keys():\n",
        "    df = pd.DataFrame({'MAE': df_dict[key].to_list(), 'Dataset': datasets})\n",
        "    df.to_csv('./experiment_tables/new_experiment_tables/experiment_table_' + str(key) + '.csv', index = False)\n",
        "processed_datasets_df = pd.read_csv('./experiment_tables/processed_datasets.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5826ga0UUBL3"
      },
      "source": [
        "# experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "BhZgNrPltXXr",
        "outputId": "d2f965f0-c904-4104-e02c-f65bf0392c45"
      },
      "outputs": [],
      "source": [
        "# rf_reg = RandomForestRegressor()\n",
        "# xgb_reg = xgb.XGBRegressor()\n",
        "# svr_reg = svm.SVR()\n",
        "\n",
        "# instance_len = len(tableList[0][0])\n",
        "# rf_results = {}\n",
        "# xgb_results = {}\n",
        "# svr_results = {}\n",
        "# cat_results = {}\n",
        "\n",
        "# j = 0\n",
        "# for (X, y) in tableList:\n",
        "#   rf_results_list = []\n",
        "#   xgb_results_list = []\n",
        "#   svr_results_list = []\n",
        "#   cat_results_list = []\n",
        "#   algName = algList[j]\n",
        "#   j += 1\n",
        "\n",
        "\n",
        "#   for i in range(0, len(X)):\n",
        "#     X_train = np.delete(X, i, 0)\n",
        "#     y_train = np.delete(y, i, 0)\n",
        "\n",
        "#     X_test = X[i]\n",
        "#     X_test = X_test.reshape(1, -1)\n",
        "#     y_test = y[i]\n",
        "\n",
        "#     rf_reg.fit(X_train, y_train)\n",
        "#     xgb_reg.fit(X_train, y_train)\n",
        "#     svr_reg.fit(X_train, y_train)\n",
        "  \n",
        "#     rf_abs_error = rf_reg.predict(X_test)\n",
        "#     xgb_abs_error = xgb_reg.predict(X_test)\n",
        "#     svr_abs_error = svr_reg.predict(X_test)\n",
        "\n",
        "\n",
        "#     rf_results_list.append([y_test, rf_abs_error[0]])\n",
        "#     xgb_results_list.append([y_test, xgb_abs_error[0]])\n",
        "#     svr_results_list.append([y_test, svr_abs_error[0]])\n",
        "\n",
        "\n",
        "#   rf_results[algName] = rf_results_list\n",
        "#   xgb_results[algName] = xgb_results_list\n",
        "#   svr_results[algName] = svr_results_list\n",
        "\n",
        "\n",
        "# # # # RANDOM FORESTS\n",
        "# # # #\n",
        "# data = []\n",
        "# cols = []\n",
        "# for key in rf_results:\n",
        "#   cols.append('abs-error-'+key)\n",
        "#   cols.append('abs-error-'+key+'-predicted')\n",
        "# cols.append('abs-error-ideal')\n",
        "# cols.append('quantifier-ideal')\n",
        "# cols.append('quantifier-ideal-num')\n",
        "# cols.append('abs-error-recommended')\n",
        "# cols.append('quantifier-recommended')\n",
        "# cols.append('quantifier-recommended-num')\n",
        "# i = 1\n",
        "# for key in rf_results:\n",
        "#     cols.append('rank-' + str(i))\n",
        "#     i += 1\n",
        "\n",
        "\n",
        "# i = 0\n",
        "# for i in range(0, instance_len):\n",
        "#   abs_error_ideal = 2\n",
        "#   quantifier_ideal = 'NULL'\n",
        "#   quantifier_ideal_num = -1\n",
        "#   abs_error_recommended = 2\n",
        "#   quantifier_recommended = 'NULL'\n",
        "#   quantifier_recommended_num = -1\n",
        "#   row = []\n",
        "#   algNum = 0\n",
        "#   rank = {}\n",
        "\n",
        "#   for a in algList:\n",
        "#     row.append(rf_results[a][i][0])\n",
        "#     row.append(rf_results[a][i][1])\n",
        "\n",
        "#     rank[algNum] = rf_results[a][i][1]\n",
        "\n",
        "#     if rf_results[a][i][0] < abs_error_ideal:\n",
        "#       abs_error_ideal = rf_results[a][i][0]\n",
        "#       quantifier_ideal = a\n",
        "#       quantifier_ideal_num = algNum\n",
        "\n",
        "#     if rf_results[a][i][1] < abs_error_recommended:\n",
        "#       abs_error_recommended = rf_results[a][i][1]\n",
        "#       quantifier_recommended = a\n",
        "#       quantifier_recommended_num = algNum\n",
        "\n",
        "#     algNum += 1\n",
        "#   rank = sorted(rank.items(), key=lambda item: item[1])\n",
        "\n",
        "#   row.append(abs_error_ideal)\n",
        "#   row.append(quantifier_ideal)\n",
        "#   row.append(quantifier_ideal_num)\n",
        "#   row.append(abs_error_recommended)\n",
        "#   row.append(quantifier_recommended)\n",
        "#   row.append(quantifier_recommended_num)\n",
        "#   for key in rank:\n",
        "#     row.append(int(key[0]))\n",
        "\n",
        "#   data.append(row)\n",
        "\n",
        "# rf_table = pd.DataFrame(data, columns = cols)\n",
        "\n",
        "\n",
        "\n",
        "# # # # XGBOOST\n",
        "# # # #\n",
        "# data = []\n",
        "# cols = []\n",
        "# for key in xgb_results:\n",
        "#   cols.append('abs-error-'+key)\n",
        "#   cols.append('abs-error-'+key+'-predicted')\n",
        "# cols.append('abs-error-ideal')\n",
        "# cols.append('quantifier-ideal')\n",
        "# cols.append('quantifier-ideal-num')\n",
        "# cols.append('abs-error-recommended')\n",
        "# cols.append('quantifier-recommended')\n",
        "# cols.append('quantifier-recommended-num')\n",
        "# i = 1\n",
        "# for key in xgb_results:\n",
        "#     cols.append('rank-' + str(i))\n",
        "#     i += 1\n",
        "\n",
        "# i = 0\n",
        "# for i in range(0, instance_len):\n",
        "#   abs_error_ideal = 2\n",
        "#   quantifier_ideal = 'NULL'\n",
        "#   quantifier_ideal_num = -1\n",
        "#   abs_error_recommended = 2\n",
        "#   quantifier_recommended = 'NULL'\n",
        "#   quantifier_recommended_num = -1\n",
        "#   row = []\n",
        "#   algNum = 0\n",
        "#   rank = {}\n",
        "\n",
        "#   for a in algList:\n",
        "#     row.append(xgb_results[a][i][0])\n",
        "#     row.append(xgb_results[a][i][1])\n",
        "\n",
        "#     rank[algNum] = xgb_results[a][i][1]\n",
        "\n",
        "#     if xgb_results[a][i][0] < abs_error_ideal:\n",
        "#       abs_error_ideal = xgb_results[a][i][0]\n",
        "#       quantifier_ideal = a\n",
        "#       quantifier_ideal_num = algNum\n",
        "\n",
        "#     if xgb_results[a][i][1] < abs_error_recommended:\n",
        "#       abs_error_recommended = xgb_results[a][i][1]\n",
        "#       quantifier_recommended = a\n",
        "#       quantifier_recommended_num = algNum\n",
        "\n",
        "#     algNum += 1\n",
        "#   rank = sorted(rank.items(), key=lambda item: item[1])\n",
        "\n",
        "#   row.append(abs_error_ideal)\n",
        "#   row.append(quantifier_ideal)\n",
        "#   row.append(quantifier_ideal_num)\n",
        "#   row.append(abs_error_recommended)\n",
        "#   row.append(quantifier_recommended)\n",
        "#   row.append(quantifier_recommended_num)\n",
        "#   for key in rank:\n",
        "#     row.append(int(key[0]))\n",
        "\n",
        "#   data.append(row)\n",
        "# xgb_table = pd.DataFrame(data, columns = cols)\n",
        "\n",
        "\n",
        "\n",
        "# # # # SVM\n",
        "# # # #\n",
        "# data = []\n",
        "# cols = []\n",
        "# for key in svr_results:\n",
        "#   cols.append('abs-error-'+key)\n",
        "#   cols.append('abs-error-'+key+'-predicted')\n",
        "# cols.append('abs-error-ideal')\n",
        "# cols.append('quantifier-ideal')\n",
        "# cols.append('quantifier-ideal-num')\n",
        "# cols.append('abs-error-recommended')\n",
        "# cols.append('quantifier-recommended')\n",
        "# cols.append('quantifier-recommended-num')\n",
        "# i = 1\n",
        "# for key in svr_results:\n",
        "#     cols.append('rank-' + str(i))\n",
        "#     i += 1\n",
        "\n",
        "# i = 0\n",
        "# for i in range(0, instance_len):\n",
        "#   abs_error_ideal = 2\n",
        "#   quantifier_ideal = 'NULL'\n",
        "#   quantifier_ideal_num = -1\n",
        "#   abs_error_recommended = 2\n",
        "#   quantifier_recommended = 'NULL'\n",
        "#   quantifier_recommended_num = -1\n",
        "#   row = []\n",
        "#   algNum = 0\n",
        "#   rank = {}\n",
        "\n",
        "#   for a in algList:\n",
        "#     row.append(svr_results[a][i][0])\n",
        "#     row.append(svr_results[a][i][1])\n",
        "\n",
        "#     rank[algNum] = svr_results[a][i][1]\n",
        "\n",
        "#     if svr_results[a][i][0] < abs_error_ideal:\n",
        "#       abs_error_ideal = svr_results[a][i][0]\n",
        "#       quantifier_ideal = a\n",
        "#       quantifier_ideal_num = algNum\n",
        "\n",
        "#     if svr_results[a][i][1] < abs_error_recommended:\n",
        "#       abs_error_recommended = svr_results[a][i][1]\n",
        "#       quantifier_recommended = a\n",
        "#       quantifier_recommended_num = algNum\n",
        "\n",
        "#     algNum += 1\n",
        "#   rank = sorted(rank.items(), key=lambda item: item[1])\n",
        "\n",
        "#   row.append(abs_error_ideal)\n",
        "#   row.append(quantifier_ideal)\n",
        "#   row.append(quantifier_ideal_num)\n",
        "#   row.append(abs_error_recommended)\n",
        "#   row.append(quantifier_recommended)\n",
        "#   row.append(quantifier_recommended_num)\n",
        "#   for key in rank:\n",
        "#     row.append(int(key[0]))\n",
        "\n",
        "#   data.append(row)\n",
        "# svr_table = pd.DataFrame(data, columns = cols)\n",
        "\n",
        "# # # rf_table.to_csv(\"./recommendation/recommendation_table_rf.csv\", index = False)\n",
        "# # # xgb_table.to_csv(\"./recommendation/recommendation_table_xgb.csv\", index = False)\n",
        "# # # svr_table.to_csv(\"./recommendation/recommendation_table_svr.csv\", index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # rf_table.to_csv(\"./recommendation/recommendation_table_rf.csv\", index = False)\n",
        "# # xgb_table.to_csv(\"./recommendation/recommendation_table_xgb.csv\", index = False)\n",
        "# # svr_table.to_csv(\"./recommendation/recommendation_table_svr.csv\", index = False)\n",
        "\n",
        "rf_table = pd.read_csv(\"./recommendation/recommendation_table_rf.csv\")\n",
        "xgb_table = pd.read_csv(\"./recommendation/recommendation_table_xgb.csv\")\n",
        "svr_table = pd.read_csv(\"./recommendation/recommendation_table_svr.csv\")\n",
        "# cat_table = pd.read_csv(\"./recommendation/recommendation_table_cat.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_dataframe = pd.DataFrame(columns=['dataset', 'regressor', 'regressor'])\n",
        "\n",
        "rf_algs = []\n",
        "xgb_algs = []\n",
        "svr_algs = []\n",
        "\n",
        "for rank in rf_table['rank-1'].tolist():\n",
        "    rf_algs.append(algList[rank])\n",
        "\n",
        "for rank in xgb_table['rank-1'].tolist():\n",
        "    xgb_algs.append(algList[rank])\n",
        "\n",
        "for rank in svr_table['rank-1'].tolist():\n",
        "    svr_algs.append(algList[rank])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# friedmann + holm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# algList.remove('T50')\n",
        "\n",
        "test_dict = {}\n",
        "optimal_dict = {}\n",
        "for d in datasets:\n",
        "    test_dict[d.split('.csv')[0]] = None\n",
        "    optimal_dict[d.split('.csv')[0]] = None\n",
        "\n",
        "for dataset in test_dict.keys():\n",
        "    counter_error_dict = {key: [] for key in algList}\n",
        "\n",
        "    for alg in algList:\n",
        "        temp_df = experiment_tables_dict[alg].loc[experiment_tables_dict[alg]['dataset_name'] == dataset]\n",
        "        temp_df = temp_df.groupby('alpha').mean(numeric_only = True)\n",
        "        temp_df = temp_df.reset_index()\n",
        "        counter_error_dict[alg] = temp_df['abs_error'].tolist()\n",
        "    test_dict[dataset] = pd.DataFrame()\n",
        "    for key in counter_error_dict.keys():\n",
        "        test_dict[dataset][key] = counter_error_dict[key]\n",
        "\n",
        "test_index = 0\n",
        "for dataset in test_dict.keys():\n",
        "    groups = []\n",
        "    for counter in test_dict[dataset].columns.tolist():\n",
        "        groups.append(test_dict[dataset][counter].tolist())\n",
        "    res = friedmanchisquare(*groups)\n",
        "    if res.pvalue < 0.05:\n",
        "        # HOLM\n",
        "        best = test_dict[dataset].mean().sort_values().index[0]\n",
        "        best_index = test_dict[dataset].columns.tolist().index(best)\n",
        "        opt_set = set()\n",
        "        opt_set.add(best)\n",
        "\n",
        "        res = posthoc_ttest(groups, p_adjust='holm')\n",
        "        \n",
        "        index = 0\n",
        "        for element in res[best_index+1]:\n",
        "            if element >= 0.05:\n",
        "                opt_set.add(test_dict[dataset].columns.tolist()[index])\n",
        "            index += 1\n",
        "        \n",
        "        optimal_dict[dataset] = opt_set\n",
        "    else:\n",
        "        optimal_dict[dataset] = set(algList)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# top k analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['abs-error-CC', 'abs-error-CC-predicted', 'abs-error-ACC',\n",
              "       'abs-error-ACC-predicted', 'abs-error-PACC', 'abs-error-PACC-predicted',\n",
              "       'abs-error-PCC', 'abs-error-PCC-predicted', 'abs-error-SMM',\n",
              "       'abs-error-SMM-predicted', 'abs-error-HDy', 'abs-error-HDy-predicted',\n",
              "       'abs-error-DyS', 'abs-error-DyS-predicted', 'abs-error-SORD',\n",
              "       'abs-error-SORD-predicted', 'abs-error-MS', 'abs-error-MS-predicted',\n",
              "       'abs-error-MS2', 'abs-error-MS2-predicted', 'abs-error-MAX',\n",
              "       'abs-error-MAX-predicted', 'abs-error-X', 'abs-error-X-predicted',\n",
              "       'abs-error-ideal', 'quantifier-ideal', 'quantifier-ideal-num',\n",
              "       'abs-error-recommended', 'quantifier-recommended',\n",
              "       'quantifier-recommended-num', 'rank-1', 'rank-2', 'rank-3', 'rank-4',\n",
              "       'rank-5', 'rank-6', 'rank-7', 'rank-8', 'rank-9', 'rank-10', 'rank-11',\n",
              "       'rank-12'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rf_table.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "dt_list = []\n",
        "for d in datasets:\n",
        "    dt_list.append(d.split('.csv')[0])\n",
        "\n",
        "rf_top_dict = {key: None for key in dt_list}\n",
        "xgb_top_dict = {key: None for key in dt_list}\n",
        "svr_top_dict = {key: None for key in dt_list}\n",
        "\n",
        "rf_ensemble_top_dict = {}\n",
        "xgb_ensemble_top_dict = {}\n",
        "svr_ensemble_top_dict = {}\n",
        "for key in dt_list:\n",
        "    rf_ensemble_top_dict[key] = []\n",
        "    xgb_ensemble_top_dict[key] = []\n",
        "    svr_ensemble_top_dict[key] = []\n",
        "\n",
        "\n",
        "rf_top_hit_list = []\n",
        "xgb_top_hit_list = []\n",
        "svr_top_hit_list = []\n",
        "\n",
        "\n",
        "top = 3\n",
        "\n",
        "\n",
        "index = 0\n",
        "for dataset in rf_top_dict.keys():\n",
        "    quantifier_set = set()\n",
        "    ensemble_quantifier_list = []\n",
        "    ensemble_error_list = []\n",
        "    for i in range(0, top):\n",
        "        quantifier_set.add(algList[rf_table.loc[index]['rank-'+str(i+1)]])\n",
        "        ensemble_quantifier_list.append(algList[rf_table.loc[index]['rank-'+str(i+1)]])\n",
        "        ensemble_error_list.append(rf_table.loc[index]['abs-error-'+str( algList[rf_table.loc[index]['rank-'+str(i+1)]] )+'-predicted'])\n",
        "    rf_top_dict[dataset] = quantifier_set\n",
        "    \n",
        "    ensemble_numerator_list = []\n",
        "    denominator = 0\n",
        "    for error in ensemble_error_list:\n",
        "        denominator += 1/error\n",
        "        ensemble_numerator_list.append( 1/error )\n",
        "    \n",
        "    ensemble_weight_list = []\n",
        "    for numerator in ensemble_numerator_list:\n",
        "        ensemble_weight_list.append( numerator / denominator )\n",
        "\n",
        "    rf_ensemble_top_dict[dataset].append(ensemble_quantifier_list)\n",
        "    rf_ensemble_top_dict[dataset].append(ensemble_weight_list)\n",
        "    \n",
        "    index += 1\n",
        "\n",
        "index = 0\n",
        "for dataset in xgb_top_dict.keys():\n",
        "    quantifier_set = set()\n",
        "    ensemble_quantifier_list = []\n",
        "    ensemble_error_list = []\n",
        "    for i in range(0, top):\n",
        "        quantifier_set.add(algList[xgb_table.loc[index]['rank-'+str(i+1)]])\n",
        "        ensemble_quantifier_list.append(algList[xgb_table.loc[index]['rank-'+str(i+1)]])\n",
        "        ensemble_error_list.append(xgb_table.loc[index]['abs-error-'+str( algList[xgb_table.loc[index]['rank-'+str(i+1)]] )+'-predicted'])\n",
        "    xgb_top_dict[dataset] = quantifier_set\n",
        "    \n",
        "    ensemble_numerator_list = []\n",
        "    denominator = 0\n",
        "    for error in ensemble_error_list:\n",
        "        denominator += 1/error\n",
        "        ensemble_numerator_list.append( 1/error )\n",
        "    \n",
        "    ensemble_weight_list = []\n",
        "    for numerator in ensemble_numerator_list:\n",
        "        ensemble_weight_list.append( numerator / denominator )\n",
        "\n",
        "    xgb_ensemble_top_dict[dataset].append(ensemble_quantifier_list)\n",
        "    xgb_ensemble_top_dict[dataset].append(ensemble_weight_list)\n",
        "    \n",
        "    index += 1\n",
        "\n",
        "index = 0\n",
        "for dataset in svr_top_dict.keys():\n",
        "    quantifier_set = set()\n",
        "    ensemble_quantifier_list = []\n",
        "    ensemble_error_list = []\n",
        "    for i in range(0, top):\n",
        "        quantifier_set.add(algList[svr_table.loc[index]['rank-'+str(i+1)]])\n",
        "        ensemble_quantifier_list.append(algList[svr_table.loc[index]['rank-'+str(i+1)]])\n",
        "        ensemble_error_list.append(svr_table.loc[index]['abs-error-'+str( algList[svr_table.loc[index]['rank-'+str(i+1)]] )+'-predicted'])\n",
        "    svr_top_dict[dataset] = quantifier_set\n",
        "    \n",
        "    ensemble_numerator_list = []\n",
        "    denominator = 0\n",
        "    for error in ensemble_error_list:\n",
        "        denominator += 1/error\n",
        "        ensemble_numerator_list.append( 1/error )\n",
        "    \n",
        "    ensemble_weight_list = []\n",
        "    for numerator in ensemble_numerator_list:\n",
        "        ensemble_weight_list.append( numerator / denominator )\n",
        "\n",
        "    svr_ensemble_top_dict[dataset].append(ensemble_quantifier_list)\n",
        "    svr_ensemble_top_dict[dataset].append(ensemble_weight_list)\n",
        "    \n",
        "    index += 1\n",
        "\n",
        "\n",
        "# index = 0\n",
        "# for dataset in xgb_top_dict.keys():\n",
        "#     quantifier_set = set()\n",
        "#     for i in range(0, top):\n",
        "#         quantifier_set.add(algList[xgb_table.loc[index]['rank-'+str(i+1)]])\n",
        "#     xgb_top_dict[dataset] = quantifier_set\n",
        "#     index += 1\n",
        "\n",
        "# index = 0\n",
        "# for dataset in svr_top_dict.keys():\n",
        "#     quantifier_set = set()\n",
        "#     for i in range(0, top):\n",
        "#         quantifier_set.add(algList[svr_table.loc[index]['rank-'+str(i+1)]])\n",
        "#     svr_top_dict[dataset] = quantifier_set\n",
        "#     index += 1\n",
        "\n",
        "\n",
        "\n",
        "index = 0\n",
        "for dataset in dt_list:\n",
        "    if len(rf_top_dict[dataset].intersection(optimal_dict[dataset])) != 0:\n",
        "        rf_top_hit_list.append(1)\n",
        "    else:\n",
        "        rf_top_hit_list.append(0)\n",
        "    index += 1\n",
        "\n",
        "\n",
        "index = 0\n",
        "for dataset in dt_list:\n",
        "    if len(xgb_top_dict[dataset].intersection(optimal_dict[dataset])) != 0:\n",
        "        xgb_top_hit_list.append(1)\n",
        "    else:\n",
        "        xgb_top_hit_list.append(0)\n",
        "    index += 1\n",
        "\n",
        "\n",
        "index = 0\n",
        "for dataset in dt_list:\n",
        "    if len(svr_top_dict[dataset].intersection(optimal_dict[dataset])) != 0:\n",
        "        svr_top_hit_list.append(1)\n",
        "    else:\n",
        "        svr_top_hit_list.append(0)\n",
        "    index += 1\n",
        "\n",
        "rf_top_hit_rate = np.mean(rf_top_hit_list)\n",
        "xgb_top_hit_rate = np.mean(xgb_top_hit_list)\n",
        "svr_top_hit_rate = np.mean(svr_top_hit_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "temp_table = pd.read_csv('./recommender_hit_table/recommender_hit_rate_table.csv')\n",
        "temp_table['Random Forest+Top5 HitRate'] = rf_top_hit_rate\n",
        "temp_table['XGBOOST+Top5 HitRate'] = xgb_top_hit_rate\n",
        "temp_table['SVM+Top5 HitRate'] = svr_top_hit_rate\n",
        "# temp_table.to_csv('./recommender_hit_table/recommender_hit_rate_table.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9473684210526315"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rf_top_hit_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# top k abs error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load_path = './experiment_tables/new_experiment_tables/'\n",
        "# experiment_table = pd.DataFrame(columns=['Method', 'Dataset', 'MAE'])\n",
        "# dt_list = []\n",
        "# for d in datasets:\n",
        "#     dt_list.append(d.split('.csv')[0])\n",
        "\n",
        "# for c in algList:\n",
        "#     df = pd.read_csv(load_path + 'experiment_table_'+str(c)+'.csv')\n",
        "#     index = 0\n",
        "#     for row in df.iterrows():\n",
        "#         experiment_table.loc[len(experiment_table)] = [c, dt_list[index], row[1]['MAE']]\n",
        "#         index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "experiment_table = pd.read_csv('./experiment_tables/def_experiment_tables/experiment_table.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "hh = pd.read_csv('./recommender_hit_table/recommender_hit_rate_table.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Random Forest HitRate</th>\n",
              "      <th>XGBOOST HitRate</th>\n",
              "      <th>SVM HitRate</th>\n",
              "      <th>Random Forest+Top3 HitRate</th>\n",
              "      <th>XGBOOST+Top3 HitRate</th>\n",
              "      <th>SVM+Top3 HitRate</th>\n",
              "      <th>Random Forest+Top5 HitRate</th>\n",
              "      <th>XGBOOST+Top5 HitRate</th>\n",
              "      <th>SVM+Top5 HitRate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.766917</td>\n",
              "      <td>0.759398</td>\n",
              "      <td>0.759398</td>\n",
              "      <td>0.917293</td>\n",
              "      <td>0.924812</td>\n",
              "      <td>0.887218</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.969925</td>\n",
              "      <td>0.954887</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Random Forest HitRate  XGBOOST HitRate  SVM HitRate  \\\n",
              "0               0.766917         0.759398     0.759398   \n",
              "\n",
              "   Random Forest+Top3 HitRate  XGBOOST+Top3 HitRate  SVM+Top3 HitRate  \\\n",
              "0                    0.917293              0.924812          0.887218   \n",
              "\n",
              "   Random Forest+Top5 HitRate  XGBOOST+Top5 HitRate  SVM+Top5 HitRate  \n",
              "0                    0.947368              0.969925          0.954887  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "methods = ['Random Forests+Top1', 'XGBOOST+Top1', 'SVM+Top1', 'Random Forests+Top3', 'XGBOOST+Top3', 'SVM+Top3', 'Random Forests+Top5', 'XGBOOST+Top5', 'SVM+Top5']\n",
        "hitrate = hh.loc[0].values.tolist()\n",
        "\n",
        "new_hit_table = pd.DataFrame({'Method': methods, 'HitRate': hitrate})\n",
        "\n",
        "new_hit_table.loc[len(new_hit_table)] = ['Random Forests+Top3+Weighted (ENSEMBLE)', hitrate[3]]\n",
        "new_hit_table.loc[len(new_hit_table)] = ['XGBOOST+Top3+Weighted (ENSEMBLE)', hitrate[4]]\n",
        "new_hit_table.loc[len(new_hit_table)] = ['SVM+Top3+Weighted (ENSEMBLE)', hitrate[5]]\n",
        "new_hit_table.loc[len(new_hit_table)] = ['Random Forests+Top5+Weighted (ENSEMBLE)', hitrate[6]]\n",
        "new_hit_table.loc[len(new_hit_table)] = ['XGBOOST+Top5+Weighted (ENSEMBLE)', hitrate[7]]\n",
        "new_hit_table.loc[len(new_hit_table)] = ['SVM+Top5+Weighted (ENSEMBLE)', hitrate[8]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# new_hit_table.to_csv('./recommender_hit_table/recommender_hit_rate_table.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# top k weighted ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "dt_list = []\n",
        "for d in datasets:\n",
        "    dt_list.append(d.split('.csv')[0])\n",
        "\n",
        "index = 0\n",
        "for item in rf_ensemble_top_dict.items():\n",
        "    quantifier_list = item[1][0]\n",
        "    weight_list = item[1][1]\n",
        "    prevalence_list = [0] * 20\n",
        "\n",
        "    real_p = None\n",
        "    for x in quantifier_list:\n",
        "        real_p = experiment_tables_dict[x].loc[experiment_tables_dict[x]['dataset_name'] == dt_list[index]].groupby('alpha').mean(numeric_only=True)['real_p'].values.tolist()\n",
        "        pred_p = experiment_tables_dict[x].loc[experiment_tables_dict[x]['dataset_name'] == dt_list[index]].groupby('alpha').mean(numeric_only=True)['pred_p'].values.tolist()\n",
        "        \n",
        "        for i in range(0, len(pred_p)):\n",
        "            prevalence_list[i] += pred_p[i] * weight_list[quantifier_list.index(x)]\n",
        "\n",
        "    ensemble_abs_error_list = []\n",
        "    for i in range(0, len(prevalence_list)):\n",
        "        ensemble_abs_error_list.append( abs( real_p[i] - prevalence_list[i] ) )\n",
        "\n",
        "    ensemble_abs_error = np.mean(ensemble_abs_error_list)\n",
        "    \n",
        "    experiment_table.loc[len(experiment_table)] = ['Random Forests+Top'+str(top)+'+Weighted (ENSEMBLE)', dt_list[index], ensemble_abs_error]\n",
        "    index += 1\n",
        "\n",
        "\n",
        "index = 0\n",
        "for item in xgb_ensemble_top_dict.items():\n",
        "    quantifier_list = item[1][0]\n",
        "    weight_list = item[1][1]\n",
        "    prevalence_list = [0] * 20\n",
        "\n",
        "    real_p = None\n",
        "    for x in quantifier_list:\n",
        "        real_p = experiment_tables_dict[x].loc[experiment_tables_dict[x]['dataset_name'] == dt_list[index]].groupby('alpha').mean(numeric_only=True)['real_p'].values.tolist()\n",
        "        pred_p = experiment_tables_dict[x].loc[experiment_tables_dict[x]['dataset_name'] == dt_list[index]].groupby('alpha').mean(numeric_only=True)['pred_p'].values.tolist()\n",
        "        \n",
        "        for i in range(0, len(pred_p)):\n",
        "            prevalence_list[i] += pred_p[i] * weight_list[quantifier_list.index(x)]\n",
        "\n",
        "    ensemble_abs_error_list = []\n",
        "    for i in range(0, len(prevalence_list)):\n",
        "        ensemble_abs_error_list.append( abs( real_p[i] - prevalence_list[i] ) )\n",
        "\n",
        "    ensemble_abs_error = np.mean(ensemble_abs_error_list)\n",
        "    \n",
        "    experiment_table.loc[len(experiment_table)] = ['XGBOOST+Top'+str(top)+'+Weighted (ENSEMBLE)', dt_list[index], ensemble_abs_error]\n",
        "    index += 1\n",
        "\n",
        "\n",
        "index = 0\n",
        "for item in svr_ensemble_top_dict.items():\n",
        "    quantifier_list = item[1][0]\n",
        "    weight_list = item[1][1]\n",
        "    prevalence_list = [0] * 20\n",
        "\n",
        "    real_p = None\n",
        "    for x in quantifier_list:\n",
        "        real_p = experiment_tables_dict[x].loc[experiment_tables_dict[x]['dataset_name'] == dt_list[index]].groupby('alpha').mean(numeric_only=True)['real_p'].values.tolist()\n",
        "        pred_p = experiment_tables_dict[x].loc[experiment_tables_dict[x]['dataset_name'] == dt_list[index]].groupby('alpha').mean(numeric_only=True)['pred_p'].values.tolist()\n",
        "        \n",
        "        for i in range(0, len(pred_p)):\n",
        "            prevalence_list[i] += pred_p[i] * weight_list[quantifier_list.index(x)]\n",
        "\n",
        "    ensemble_abs_error_list = []\n",
        "    for i in range(0, len(prevalence_list)):\n",
        "        ensemble_abs_error_list.append( abs( real_p[i] - prevalence_list[i] ) )\n",
        "\n",
        "    ensemble_abs_error = np.mean(ensemble_abs_error_list)\n",
        "    \n",
        "    experiment_table.loc[len(experiment_table)] = ['SVM+Top'+str(top)+'+Weighted (ENSEMBLE)', dt_list[index], ensemble_abs_error]\n",
        "    index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['CC', 'ACC', 'PACC', 'PCC', 'SMM', 'HDy', 'DyS', 'SORD', 'MS',\n",
              "       'MS2', 'MAX', 'X', 'Random Forests+Top3 (MEAN)',\n",
              "       'Random Forests+Top3 (MEDIAN)', 'XGBOOST+Top3 (MEAN)',\n",
              "       'XGBOOST+Top3 (MEDIAN)', 'SVM+Top3 (MEAN)', 'SVM+Top3 (MEDIAN)',\n",
              "       'Random Forests+Top5 (MEAN)', 'Random Forests+Top5 (MEDIAN)',\n",
              "       'XGBOOST+Top5 (MEAN)', 'XGBOOST+Top5 (MEDIAN)', 'SVM+Top5 (MEAN)',\n",
              "       'SVM+Top5 (MEDIAN)', 'Random Forests+Top1', 'XGBOOST+Top1',\n",
              "       'SVM+Top1', 'Random Forests+Top3+Weighted (ENSEMBLE)',\n",
              "       'XGBOOST+Top3+Weighted (ENSEMBLE)', 'SVM+Top3+Weighted (ENSEMBLE)',\n",
              "       'Random Forests+Top5+Weighted (ENSEMBLE)',\n",
              "       'XGBOOST+Top5+Weighted (ENSEMBLE)', 'SVM+Top5+Weighted (ENSEMBLE)'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "experiment_table['Method'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# experiment_table.to_csv('./experiment_tables/def_experiment_tables/experiment_table.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Method</th>\n",
              "      <th>Dataset</th>\n",
              "      <th>MAE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CC</td>\n",
              "      <td>1043_ada_agnostic</td>\n",
              "      <td>0.183200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CC</td>\n",
              "      <td>1048_jEdit_4</td>\n",
              "      <td>0.199050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CC</td>\n",
              "      <td>1049_pc4</td>\n",
              "      <td>0.288450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CC</td>\n",
              "      <td>1050_pc3</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CC</td>\n",
              "      <td>1054_mc2</td>\n",
              "      <td>0.270000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4384</th>\n",
              "      <td>SVM+Top5+Weighted (ENSEMBLE)</td>\n",
              "      <td>namao</td>\n",
              "      <td>0.007424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4385</th>\n",
              "      <td>SVM+Top5+Weighted (ENSEMBLE)</td>\n",
              "      <td>occupancy</td>\n",
              "      <td>0.004387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4386</th>\n",
              "      <td>SVM+Top5+Weighted (ENSEMBLE)</td>\n",
              "      <td>phoneme</td>\n",
              "      <td>0.025998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4387</th>\n",
              "      <td>SVM+Top5+Weighted (ENSEMBLE)</td>\n",
              "      <td>spambase</td>\n",
              "      <td>0.013023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4388</th>\n",
              "      <td>SVM+Top5+Weighted (ENSEMBLE)</td>\n",
              "      <td>winetype</td>\n",
              "      <td>0.004340</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4389 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                            Method            Dataset       MAE\n",
              "0                               CC  1043_ada_agnostic  0.183200\n",
              "1                               CC       1048_jEdit_4  0.199050\n",
              "2                               CC           1049_pc4  0.288450\n",
              "3                               CC           1050_pc3  0.500000\n",
              "4                               CC           1054_mc2  0.270000\n",
              "...                            ...                ...       ...\n",
              "4384  SVM+Top5+Weighted (ENSEMBLE)              namao  0.007424\n",
              "4385  SVM+Top5+Weighted (ENSEMBLE)          occupancy  0.004387\n",
              "4386  SVM+Top5+Weighted (ENSEMBLE)            phoneme  0.025998\n",
              "4387  SVM+Top5+Weighted (ENSEMBLE)           spambase  0.013023\n",
              "4388  SVM+Top5+Weighted (ENSEMBLE)           winetype  0.004340\n",
              "\n",
              "[4389 rows x 3 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "experiment_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dt_list = []\n",
        "for d in datasets:\n",
        "    dt_list.append(d.split('.csv')[0])\n",
        "\n",
        "index = 0\n",
        "for item in rf_top_dict.items():\n",
        "    quantifiers_set = item[1]\n",
        "    \n",
        "    pred_p_mean = [0] * 20\n",
        "    temp_pred_p_median = [ [] for _ in range(20) ]\n",
        "    pred_p_median = []\n",
        "    real_p = None\n",
        "    for x in quantifier_set:\n",
        "        real_p = experiment_tables_dict[x].loc[experiment_tables_dict[x]['dataset_name'] == dt_list[index]].groupby('alpha').mean(numeric_only=True)['real_p'].values.tolist()\n",
        "\n",
        "        pred_p = experiment_tables_dict[x].loc[experiment_tables_dict[x]['dataset_name'] == dt_list[index]].groupby('alpha').mean(numeric_only=True)['pred_p'].values.tolist()\n",
        "        for i in range(0, len(real_p)):\n",
        "            pred_p_mean[i] += pred_p[i]\n",
        "            temp_pred_p_median[i].append(pred_p[i])\n",
        "    \n",
        "    for i in range(0, len(pred_p_mean)):\n",
        "        pred_p_mean[i] /= len(quantifier_set)\n",
        "        pred_p_median.append(np.median(temp_pred_p_median[i]))\n",
        "    \n",
        "    abs_error_mean_list = []\n",
        "    abs_error_median_list = []\n",
        "    for i in range(0, len(pred_p_mean)):\n",
        "        abs_error_mean_list.append(abs(real_p[i] - pred_p_mean[i]))\n",
        "        abs_error_median_list.append(abs(real_p[i] - pred_p_median[i]))\n",
        "    abs_error_mean = np.mean(abs_error_mean_list)\n",
        "    abs_error_median = np.mean(abs_error_median_list)\n",
        "\n",
        "    experiment_table.loc[len(experiment_table)] = ['Random Forests+Top1 - MEAN', dt_list[index], abs_error_mean]\n",
        "    experiment_table.loc[len(experiment_table)] = ['Random Forests+Top1 - MEDIAN', dt_list[index], abs_error_median]\n",
        "\n",
        "    index += 1\n",
        "\n",
        "\n",
        "dt_list = []\n",
        "for d in datasets:\n",
        "    dt_list.append(d.split('.csv')[0])\n",
        "\n",
        "index = 0\n",
        "for item in xgb_top_dict.items():\n",
        "    quantifiers_set = item[1]\n",
        "    \n",
        "    pred_p_mean = [0] * 20\n",
        "    temp_pred_p_median = [ [] for _ in range(20) ]\n",
        "    pred_p_median = []\n",
        "    real_p = None\n",
        "    for x in quantifier_set:\n",
        "        real_p = experiment_tables_dict[x].loc[experiment_tables_dict[x]['dataset_name'] == dt_list[index]].groupby('alpha').mean(numeric_only=True)['real_p'].values.tolist()\n",
        "\n",
        "        pred_p = experiment_tables_dict[x].loc[experiment_tables_dict[x]['dataset_name'] == dt_list[index]].groupby('alpha').mean(numeric_only=True)['pred_p'].values.tolist()\n",
        "        for i in range(0, len(real_p)):\n",
        "            pred_p_mean[i] += pred_p[i]\n",
        "            temp_pred_p_median[i].append(pred_p[i])\n",
        "    \n",
        "    for i in range(0, len(pred_p_mean)):\n",
        "        pred_p_mean[i] /= len(quantifier_set)\n",
        "        pred_p_median.append(np.median(temp_pred_p_median[i]))\n",
        "    \n",
        "    abs_error_mean_list = []\n",
        "    abs_error_median_list = []\n",
        "    for i in range(0, len(pred_p_mean)):\n",
        "        abs_error_mean_list.append(abs(real_p[i] - pred_p_mean[i]))\n",
        "        abs_error_median_list.append(abs(real_p[i] - pred_p_median[i]))\n",
        "    abs_error_mean = np.mean(abs_error_mean_list)\n",
        "    abs_error_median = np.mean(abs_error_median_list)\n",
        "\n",
        "    experiment_table.loc[len(experiment_table)] = ['XGBOOST+Top1 - MEAN', dt_list[index], abs_error_mean]\n",
        "    experiment_table.loc[len(experiment_table)] = ['XGBOOST+Top1 - MEDIAN', dt_list[index], abs_error_median]\n",
        "\n",
        "    index += 1\n",
        "\n",
        "\n",
        "dt_list = []\n",
        "for d in datasets:\n",
        "    dt_list.append(d.split('.csv')[0])\n",
        "\n",
        "index = 0\n",
        "for item in svr_top_dict.items():\n",
        "    quantifiers_set = item[1]\n",
        "    \n",
        "    pred_p_mean = [0] * 20\n",
        "    temp_pred_p_median = [ [] for _ in range(20) ]\n",
        "    pred_p_median = []\n",
        "    real_p = None\n",
        "    for x in quantifier_set:\n",
        "        real_p = experiment_tables_dict[x].loc[experiment_tables_dict[x]['dataset_name'] == dt_list[index]].groupby('alpha').mean(numeric_only=True)['real_p'].values.tolist()\n",
        "\n",
        "        pred_p = experiment_tables_dict[x].loc[experiment_tables_dict[x]['dataset_name'] == dt_list[index]].groupby('alpha').mean(numeric_only=True)['pred_p'].values.tolist()\n",
        "        for i in range(0, len(real_p)):\n",
        "            pred_p_mean[i] += pred_p[i]\n",
        "            temp_pred_p_median[i].append(pred_p[i])\n",
        "    \n",
        "    for i in range(0, len(pred_p_mean)):\n",
        "        pred_p_mean[i] /= len(quantifier_set)\n",
        "        pred_p_median.append(np.median(temp_pred_p_median[i]))\n",
        "    \n",
        "    abs_error_mean_list = []\n",
        "    abs_error_median_list = []\n",
        "    for i in range(0, len(pred_p_mean)):\n",
        "        abs_error_mean_list.append(abs(real_p[i] - pred_p_mean[i]))\n",
        "        abs_error_median_list.append(abs(real_p[i] - pred_p_median[i]))\n",
        "    abs_error_mean = np.mean(abs_error_mean_list)\n",
        "    abs_error_median = np.mean(abs_error_median_list)\n",
        "\n",
        "    experiment_table.loc[len(experiment_table)] = ['SVM+Top1 - MEAN', dt_list[index], abs_error_mean]\n",
        "    experiment_table.loc[len(experiment_table)] = ['SVM+Top1 - MEDIAN', dt_list[index], abs_error_median]\n",
        "\n",
        "    index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # experiment_table.to_csv('./experiment_tables/def_experiment_tables/experiment_table.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hit Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dt_list = []\n",
        "for d in datasets:\n",
        "    dt_list.append(d.split('.csv')[0])\n",
        "\n",
        "rf_hit_list = []\n",
        "xgb_hit_list = []\n",
        "svr_hit_list = []\n",
        "\n",
        "rf_recommended_list = rf_table['quantifier-recommended'].values.tolist()\n",
        "index = 0\n",
        "for dataset in dt_list:\n",
        "    if rf_recommended_list[index] in optimal_dict[dataset]:\n",
        "        rf_hit_list.append(1)\n",
        "    else:\n",
        "        rf_hit_list.append(0)\n",
        "    index += 1\n",
        "\n",
        "xgb_recommended_list = xgb_table['quantifier-recommended'].values.tolist()\n",
        "index = 0\n",
        "for dataset in dt_list:\n",
        "    if xgb_recommended_list[index] in optimal_dict[dataset]:\n",
        "        xgb_hit_list.append(1)\n",
        "    else:\n",
        "        xgb_hit_list.append(0)\n",
        "    index += 1\n",
        "\n",
        "svr_recommended_list = svr_table['quantifier-recommended'].values.tolist()\n",
        "index = 0\n",
        "for dataset in dt_list:\n",
        "    if svr_recommended_list[index] in optimal_dict[dataset]:\n",
        "        svr_hit_list.append(1)\n",
        "    else:\n",
        "        svr_hit_list.append(0)\n",
        "    index += 1\n",
        "\n",
        "# hit_list = rf_hit_list + xgb_hit_list + svr_hit_list\n",
        "# datasets_list = dt_list + dt_list + dt_list\n",
        "# regressor_list = ['Random Forests'] * 133 + ['XGBOOST'] * 133 + ['SVM'] * 133 + ['Random Forests+Top'+str(top)] * 133 + ['XGBOOST+Top'+str(top)] * 133 + ['SVM+Top'+str(top)] * 133\n",
        "# quantifier_recommended_list = rf_recommended_list + xgb_recommended_list + svr_recommended_list\n",
        "\n",
        "# recommender_hit_df = pd.DataFrame({'dataset': datasets_list, 'regressor': regressor_list, 'quantifier_recommended': quantifier_recommended_list, 'hit': hit_list})\n",
        "# recommender_hit_df.to_csv('./recommender_hit_table/recommender_hit_table.csv', index = False)\n",
        "# hit_rate_table = pd.DataFrame(columns=['Random Forests HitRate', 'XGBOOST HitRate', 'SVM HitRate', 'Random Forests+Top'+str(top)+' HitRate', 'XGBOOST+Top'+str(top)+' HitRate', 'SVM HitRate+Top'+str(top)+' HitRate'])\n",
        "# hit_rate_table.loc[len(hit_rate_table)] = [np.mean(rf_hit_list), np.mean(xgb_hit_list), np.mean(xgb_hit_list)]\n",
        "# hit_rate_table.to_csv('./recommender_hit_table/recommender_hit_rate_table.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_datasets_df = pd.read_csv('./experiment_tables/processed_datasets.csv')\n",
        "datasets = processed_datasets_df['dataset'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rows = rf_table['abs-error-DyS'].to_list()\n",
        "dys = []\n",
        "i = 0\n",
        "for r in rows:\n",
        "    dys.append(rf_table.loc[i]['abs-error-DyS'])\n",
        "    i += 1\n",
        "\n",
        "\n",
        "rows = rf_table['quantifier-recommended'].to_list()\n",
        "rec = []\n",
        "i = 0\n",
        "for r in rows:\n",
        "    rec.append(rf_table.loc[i]['abs-error-' + r])\n",
        "    i += 1\n",
        "\n",
        "rows = rf_table['quantifier-ideal'].to_list()\n",
        "rec2 = []\n",
        "i = 0\n",
        "for r in rows:\n",
        "    rec2.append(rf_table.loc[i]['abs-error-' + r])\n",
        "    i += 1\n",
        "\n",
        "rows = rf_table['quantifier-ideal'].to_list()\n",
        "rec3 = []\n",
        "i = 0\n",
        "for r in rows:\n",
        "    rec3.append(rf_table.loc[i]['abs-error-' + random.choice(algList)])\n",
        "    i += 1\n",
        "\n",
        "ranking_dict = {'rank-1': [], 'rank-2': [], 'rank-3': [], 'rank-4': [], 'rank-5': [], 'rank-6': [], 'rank-7': [], 'rank-8': [], 'rank-9': [], 'rank-10': [], 'rank-11': [], 'rank-12': [], 'rank-13': []}\n",
        "rows = rf_table['quantifier-ideal'].to_list()\n",
        "i = 0\n",
        "for r in rows:\n",
        "    for j in range(13):\n",
        "        ranking_dict['rank-'+str(j+1)].append(algList[rf_table.loc[i]['rank-'+str(j+1)]])\n",
        "    i += 1\n",
        "\n",
        "def generate_ensemble_results_rf(top, type):\n",
        "    rec = []\n",
        "    i = 0\n",
        "    for r in rows:\n",
        "        j = 0\n",
        "        error_list = []\n",
        "        for j in range(top):\n",
        "            error_list.append(rf_table.loc[i][ 'abs-error-' + ranking_dict['rank-'+str(j+1)][i] ])\n",
        "        if type == 'mean':\n",
        "            rec.append(np.mean(error_list))\n",
        "        elif type == 'median':\n",
        "            rec.append(np.median(error_list))\n",
        "        i += 1\n",
        "    return rec\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "top = 5\n",
        "rec4 = generate_ensemble_results_rf(top, 'median')\n",
        "rec5 = generate_ensemble_results_rf(top, 'mean')\n",
        "\n",
        "data = [rec2, rec3, dys, rec, rec4, rec5]\n",
        "fig = plt.figure(figsize =(15, 7))\n",
        "\n",
        "# Creating plot\n",
        "plt.boxplot(data)\n",
        "plt.xticks([1,2,3,4,5,6], ['Ideal', 'Aleatória', 'DyS', 'Recomendador', 'Ensemble - Median (Top '+ str(top) +')', 'Ensemble - Mean (Top '+ str(top) +')'])\n",
        "# plt.title('Distribuição de erros médios absolutos (MAE) por método de escolha de quantificador')\n",
        "plt.xlabel('Método de escolha do quantificador (RANDOM FOREST REGRESSOR)')\n",
        "plt.ylabel('MAE')\n",
        "# show plot\n",
        "plt.show()\n",
        "\n",
        "# error_topline = pd.DataFrame({'MAE': rec2, 'Dataset': datasets})\n",
        "# error_topline.to_csv('./recommendation/error_topline.csv', index = False)\n",
        "# error_baseline = pd.DataFrame({'MAE': rec3, 'Dataset': datasets})\n",
        "# error_baseline.to_csv('./recommendation/error_baseline.csv', index = False)\n",
        "# error_dys = pd.DataFrame({'MAE': dys, 'Dataset': datasets})\n",
        "# error_dys.to_csv('./recommendation/error_dys.csv', index = False)\n",
        "\n",
        "# error_recommender_rf = pd.DataFrame({'MAE': rec, 'Dataset': datasets})\n",
        "# error_recommender_rf.to_csv('./recommendation/error_recommender_rf.csv', index = False)\n",
        "error_ensemble_median_rf = pd.DataFrame({'MAE': rec4, 'Dataset': datasets})\n",
        "error_ensemble_median_rf.to_csv('./recommendation/error_ensemble_median_top5_rf.csv', index = False)\n",
        "error_ensemble_mean_rf = pd.DataFrame({'MAE': rec5, 'Dataset': datasets})\n",
        "error_ensemble_mean_rf.to_csv('./recommendation/error_ensemble_mean_top5_rf.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# better plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf_top_hit_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hit_rate_table = pd.read_csv('./recommender_hit_table/recommender_hit_rate_table.csv')\n",
        "hit_rates = hit_rate_table.loc[0].values.tolist()\n",
        "hit_rates = hit_rates + [rf_top_hit_rate, xgb_top_hit_rate, svr_top_hit_rate]\n",
        "hit_rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "hit_rate_table = pd.read_csv('./recommender_hit_table/recommender_hit_rate_table.csv')\n",
        "hit_rates = hit_rate_table.loc[0].values.tolist()\n",
        "hit_rates = hit_rates + [rf_top_hit_rate, xgb_top_hit_rate, svr_top_hit_rate]\n",
        "\n",
        "# Hit-rate data for each meta-learner\n",
        "meta_learners = ['Random Forests', 'XGBOOST', 'SVM', 'Random Forests+Top'+str(top), 'XGBOOST+Top'+str(top), 'SVM+Top'+str(top)]\n",
        "\n",
        "# Create the plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Create a bar plot\n",
        "bars = plt.bar(meta_learners, hit_rates, color=['b', 'g', 'r', 'c', 'm'])\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Meta-Learners')\n",
        "plt.ylabel('Hit-Rate')\n",
        "plt.title('Comparison of Meta-Learners Based on Hit-Rate')\n",
        "\n",
        "# Adding the hit-rate values on top of the bars\n",
        "for bar, hit_rate in zip(bars, hit_rates):\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, yval + 0.01, round(hit_rate, 2), ha='center', va='bottom')\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ranking_dict = {'rank-1': [], 'rank-2': [], 'rank-3': [], 'rank-4': [], 'rank-5': [], 'rank-6': [], 'rank-7': [], 'rank-8': [], 'rank-9': [], 'rank-10': [], 'rank-11': [], 'rank-12': [], 'rank-13': []}\n",
        "# rows = rf_table['quantifier-ideal'].to_list()\n",
        "# i = 0\n",
        "# for r in rows:\n",
        "#     for j in range(13):\n",
        "#         ranking_dict['rank-'+str(j+1)].append(algList[rf_table.loc[i]['rank-'+str(j+1)]])\n",
        "#     i += 1\n",
        "\n",
        "\n",
        "\n",
        "# for r in rows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# rows = xgb_table['abs-error-DyS'].to_list()\n",
        "# dys = []\n",
        "# i = 0\n",
        "# for r in rows:\n",
        "#     dys.append(xgb_table.loc[i]['abs-error-DyS'])\n",
        "#     i += 1\n",
        "\n",
        "\n",
        "rows = xgb_table['quantifier-recommended'].to_list()\n",
        "rec = []\n",
        "i = 0\n",
        "for r in rows:\n",
        "    rec.append(xgb_table.loc[i]['abs-error-' + r])\n",
        "    i += 1\n",
        "\n",
        "# rows = xgb_table['quantifier-ideal'].to_list()\n",
        "# rec2 = []\n",
        "# i = 0\n",
        "# for r in rows:\n",
        "#     rec2.append(xgb_table.loc[i]['abs-error-' + r])\n",
        "#     i += 1\n",
        "\n",
        "# rows = xgb_table['quantifier-ideal'].to_list()\n",
        "# rec3 = []\n",
        "# i = 0\n",
        "# for r in rows:\n",
        "#     rec3.append(xgb_table.loc[i]['abs-error-' + random.choice(algList)])\n",
        "#     i += 1\n",
        "\n",
        "ranking_dict = {'rank-1': [], 'rank-2': [], 'rank-3': [], 'rank-4': [], 'rank-5': [], 'rank-6': [], 'rank-7': [], 'rank-8': [], 'rank-9': [], 'rank-10': [], 'rank-11': [], 'rank-12': [], 'rank-13': []}\n",
        "rows = xgb_table['quantifier-ideal'].to_list()\n",
        "i = 0\n",
        "for r in rows:\n",
        "    for j in range(13):\n",
        "        ranking_dict['rank-'+str(j+1)].append(algList[xgb_table.loc[i]['rank-'+str(j+1)]])\n",
        "    i += 1\n",
        "\n",
        "def generate_ensemble_results_xgb(top, type):\n",
        "    rec = []\n",
        "    i = 0\n",
        "    for r in rows:\n",
        "        j = 0\n",
        "        error_list = []\n",
        "        for j in range(top):\n",
        "            error_list.append(xgb_table.loc[i][ 'abs-error-' + ranking_dict['rank-'+str(j+1)][i] ])\n",
        "        if type == 'mean':\n",
        "            rec.append(np.mean(error_list))\n",
        "        elif type == 'median':\n",
        "            rec.append(np.median(error_list))\n",
        "        i += 1\n",
        "    return rec\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "top = 5\n",
        "rec4 = generate_ensemble_results_xgb(top, 'median')\n",
        "rec5 = generate_ensemble_results_xgb(top, 'mean')\n",
        "\n",
        "data = [rec2, rec3, dys, rec, rec4, rec5]\n",
        "fig = plt.figure(figsize =(15, 7))\n",
        "\n",
        "# Creating plot\n",
        "plt.boxplot(data)\n",
        "plt.xticks([1,2,3,4,5,6], ['Ideal', 'Aleatória', 'DyS', 'Recomendador', 'Ensemble - Median (Top '+ str(top) +')', 'Ensemble - Mean (Top '+ str(top) +')'])\n",
        "# plt.title('Distribuição de erros médios absolutos (MAE) por método de escolha de quantificador')\n",
        "plt.xlabel('Método de escolha do quantificador (XGBOOST)')\n",
        "plt.ylabel('MAE')\n",
        "# show plot\n",
        "plt.show()\n",
        "\n",
        "# error_recommender_xgb = pd.DataFrame({'MAE': rec, 'Dataset': datasets})\n",
        "# error_recommender_xgb.to_csv('./recommendation/error_recommender_xgb.csv', index = False)\n",
        "error_ensemble_median_xgb = pd.DataFrame({'MAE': rec4, 'Dataset': datasets})\n",
        "error_ensemble_median_xgb.to_csv('./recommendation/error_ensemble_median_top5_xgb.csv', index = False)\n",
        "error_ensemble_mean_xgb = pd.DataFrame({'MAE': rec5, 'Dataset': datasets})\n",
        "error_ensemble_mean_xgb.to_csv('./recommendation/error_ensemble_mean_top5_xgb.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# rows = svr_table['abs-error-DyS'].to_list()\n",
        "# dys = []\n",
        "# i = 0\n",
        "# for r in rows:\n",
        "#     dys.append(svr_table.loc[i]['abs-error-DyS'])\n",
        "#     i += 1\n",
        "\n",
        "\n",
        "rows = svr_table['quantifier-recommended'].to_list()\n",
        "rec = []\n",
        "i = 0\n",
        "for r in rows:\n",
        "    rec.append(svr_table.loc[i]['abs-error-' + r])\n",
        "    i += 1\n",
        "\n",
        "# rows = svr_table['quantifier-ideal'].to_list()\n",
        "# rec2 = []\n",
        "# i = 0\n",
        "# for r in rows:\n",
        "#     rec2.append(svr_table.loc[i]['abs-error-' + r])\n",
        "#     i += 1\n",
        "\n",
        "# rows = svr_table['quantifier-ideal'].to_list()\n",
        "# rec3 = []\n",
        "# i = 0\n",
        "# for r in rows:\n",
        "#     rec3.append(svr_table.loc[i]['abs-error-' + random.choice(algList)])\n",
        "#     i += 1\n",
        "\n",
        "ranking_dict = {'rank-1': [], 'rank-2': [], 'rank-3': [], 'rank-4': [], 'rank-5': [], 'rank-6': [], 'rank-7': [], 'rank-8': [], 'rank-9': [], 'rank-10': [], 'rank-11': [], 'rank-12': [], 'rank-13': []}\n",
        "rows = svr_table['quantifier-ideal'].to_list()\n",
        "i = 0\n",
        "for r in rows:\n",
        "    for j in range(13):\n",
        "        ranking_dict['rank-'+str(j+1)].append(algList[svr_table.loc[i]['rank-'+str(j+1)]])\n",
        "    i += 1\n",
        "\n",
        "def generate_ensemble_results_svr(top, type):\n",
        "    rec = []\n",
        "    i = 0\n",
        "    for r in rows:\n",
        "        j = 0\n",
        "        error_list = []\n",
        "        for j in range(top):\n",
        "            error_list.append(svr_table.loc[i][ 'abs-error-' + ranking_dict['rank-'+str(j+1)][i] ])\n",
        "        if type == 'mean':\n",
        "            rec.append(np.mean(error_list))\n",
        "        elif type == 'median':\n",
        "            rec.append(np.median(error_list))\n",
        "        i += 1\n",
        "    return rec\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "top = 5\n",
        "rec4 = generate_ensemble_results_svr(top, 'median')\n",
        "rec5 = generate_ensemble_results_svr(top, 'mean')\n",
        "data = [rec2, rec3, dys, rec, rec4, rec5]\n",
        "\n",
        "fig = plt.figure(figsize =(15, 7))\n",
        "# Creating plot\n",
        "plt.boxplot(data)\n",
        "plt.xticks([1,2,3,4,5,6], ['Ideal', 'Aleatória', 'DyS', 'Recomendador', 'Ensemble - Median (Top '+ str(top) +')', 'Ensemble - Mean (Top '+ str(top) +')'])\n",
        "# plt.title('Distribuição de erros médios absolutos (MAE) por método de escolha de quantificador')\n",
        "plt.xlabel('Método de escolha do quantificador (SUPPORT VECTOR MACHINES)')\n",
        "plt.ylabel('MAE')\n",
        "# show plot\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# error_recommender_svm = pd.DataFrame({'MAE': rec, 'Dataset': datasets})\n",
        "# error_recommender_svm.to_csv('./recommendation/error_recommender_svm.csv', index = False)\n",
        "error_ensemble_median_svm = pd.DataFrame({'MAE': rec4, 'Dataset': datasets})\n",
        "error_ensemble_median_svm.to_csv('./recommendation/error_ensemble_median_top5_svm.csv', index = False)\n",
        "error_ensemble_mean_svm = pd.DataFrame({'MAE': rec5, 'Dataset': datasets})\n",
        "error_ensemble_mean_svm.to_csv('./recommendation/error_ensemble_mean_top5_svm.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rows = cat_table['quantifier-recommended'].to_list()\n",
        "rec = []\n",
        "i = 0\n",
        "for r in rows:\n",
        "    rec.append(cat_table.loc[i]['abs-error-' + r])\n",
        "    i += 1\n",
        "\n",
        "ranking_dict = {'rank-1': [], 'rank-2': [], 'rank-3': [], 'rank-4': [], 'rank-5': [], 'rank-6': [], 'rank-7': [], 'rank-8': [], 'rank-9': [], 'rank-10': [], 'rank-11': [], 'rank-12': [], 'rank-13': []}\n",
        "rows = cat_table['quantifier-ideal'].to_list()\n",
        "i = 0\n",
        "for r in rows:\n",
        "    for j in range(13):\n",
        "        ranking_dict['rank-'+str(j+1)].append(algList[cat_table.loc[i]['rank-'+str(j+1)]])\n",
        "    i += 1\n",
        "\n",
        "def generate_ensemble_results_cat(top, type):\n",
        "    rec = []\n",
        "    i = 0\n",
        "    for r in rows:\n",
        "        j = 0\n",
        "        error_list = []\n",
        "        for j in range(top):\n",
        "            error_list.append(cat_table.loc[i][ 'abs-error-' + ranking_dict['rank-'+str(j+1)][i] ])\n",
        "        if type == 'mean':\n",
        "            rec.append(np.mean(error_list))\n",
        "        elif type == 'median':\n",
        "            rec.append(np.median(error_list))\n",
        "        i += 1\n",
        "    return rec\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "top = 3\n",
        "rec4 = generate_ensemble_results_cat(top, 'median')\n",
        "rec5 = generate_ensemble_results_cat(top, 'mean')\n",
        "data = [rec2, rec3, dys, rec, rec4, rec5]\n",
        "\n",
        "fig = plt.figure(figsize =(15, 7))\n",
        "# Creating plot\n",
        "plt.boxplot(data)\n",
        "plt.xticks([1,2,3,4,5,6], ['Ideal', 'Aleatória', 'DyS', 'Recomendador', 'Ensemble - Median (Top '+ str(top) +')', 'Ensemble - Mean (Top '+ str(top) +')'])\n",
        "# plt.title('Distribuição de erros médios absolutos (MAE) por método de escolha de quantificador')\n",
        "plt.xlabel('Método de escolha do quantificador (SUPPORT VECTOR MACHINES)')\n",
        "plt.ylabel('MAE')\n",
        "# show plot\n",
        "plt.show()\n",
        "\n",
        "\n",
        "error_recommender_cat = pd.DataFrame({'MAE': rec, 'Dataset': datasets})\n",
        "error_recommender_cat.to_csv('./recommendation/error_recommender_cat.csv', index = False)\n",
        "error_ensemble_median_cat = pd.DataFrame({'MAE': rec4, 'Dataset': datasets})\n",
        "error_ensemble_median_cat.to_csv('./recommendation/error_ensemble_median_top3_cat.csv', index = False)\n",
        "error_ensemble_mean_cat = pd.DataFrame({'MAE': rec5, 'Dataset': datasets})\n",
        "error_ensemble_mean_cat.to_csv('./recommendation/error_ensemble_mean_top3_cat.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eRcYT_jEfv9",
        "outputId": "e163d42f-c966-4bc9-a5c2-cfa9bac680b6"
      },
      "outputs": [],
      "source": [
        "# x_axis = [] \n",
        "# y_axis = []\n",
        "# top = len(pathList)\n",
        "\n",
        "# for j in range(1, top+1):\n",
        "#   ideal = table['quantifier-ideal-num'].values\n",
        "#   pred = []\n",
        "#   i = 1\n",
        "#   for i in range(1, j+1):\n",
        "#     pred.append(table['rank-'+str(i)].values.tolist())\n",
        "#   pred = np.array(pred).T.tolist()\n",
        "#   mat = []\n",
        "\n",
        "#   i = 0\n",
        "#   for row in pred:\n",
        "#     correct = False\n",
        "#     for element in row:\n",
        "#       if ideal[i] == element:\n",
        "#         correct = True\n",
        "#         break\n",
        "#     mat.append(correct)\n",
        "#     i += 1\n",
        "\n",
        "#   x_axis.append(np.count_nonzero(mat) / len(mat))\n",
        "#   y_axis.append(j)\n",
        "\n",
        "# plt.ylabel('Acurácia')\n",
        "# plt.xlabel('Top N')\n",
        "# plt.xticks([1,2,3,4,5,6,7,8,9,10])\n",
        "\n",
        "# plt.plot(y_axis, x_axis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(x_axis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# top = 1\n",
        "\n",
        "# x_axis = [0] * len(pathList)\n",
        "# y_axis = algList\n",
        "\n",
        "# for i in range(0, top):\n",
        "#     rank = table.value_counts('rank-' + str(i+1))\n",
        "#     for a in rank.index:\n",
        "#         x_axis[a] += rank[a]\n",
        "\n",
        "# x_axis = np.array(x_axis) / len(table)\n",
        "\n",
        "# fig, ax = plt.subplots()\n",
        "# ax.bar(y_axis, x_axis, width=1, edgecolor=\"white\", linewidth=0.7)\n",
        "# ax.set(xlim=(0, 9))\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# top = 3\n",
        "# auc_max = 1\n",
        "\n",
        "# table_copy = table.copy(deep=True)\n",
        "# auc = pd.read_csv(\"./auc/auc_score.csv\")\n",
        "# auc = auc['auc_score'].values.tolist()\n",
        "\n",
        "# for i in range(0, len(auc)):\n",
        "#     if auc[i] >= auc_max:\n",
        "#         table_copy.drop(i, inplace = True)\n",
        "# table_copy.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# x_axis = [0] * len(pathList)\n",
        "# y_axis = algList\n",
        "\n",
        "# for i in range(0, top):\n",
        "#     rank = table_copy.value_counts('rank-' + str(i+1))\n",
        "#     for a in rank.index:\n",
        "#         x_axis[a] += rank[a]\n",
        "\n",
        "# fig, ax = plt.subplots()\n",
        "# ax.bar(y_axis, x_axis, width=1, edgecolor=\"white\", linewidth=0.7)\n",
        "# ax.set(xlim=(0, 9))\n",
        "# plt.show()\n",
        "\n",
        "# print(len(table_copy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# x_axis = [0] * len(pathList)\n",
        "# y_axis = algList\n",
        "# qnt = table['quantifier-ideal-num'].values\n",
        "\n",
        "# for a in qnt:\n",
        "#     x_axis[a] += 1\n",
        "\n",
        "# fig, ax = plt.subplots()\n",
        "# ax.bar(y_axis, x_axis, width=1, edgecolor=\"white\", linewidth=0.7)\n",
        "# ax.set(xlim=(0, 9))\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# top = 1\n",
        "# auc_max = 1\n",
        "\n",
        "# table_copy = table.copy(deep=True)\n",
        "# auc = pd.read_csv(\"./auc/auc_score.csv\")\n",
        "# auc = auc['auc_score'].values.tolist()\n",
        "\n",
        "# for i in range(0, len(auc)):\n",
        "#     if auc[i] >= auc_max:\n",
        "#         table_copy.drop(i, inplace = True)\n",
        "# table_copy.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# x_axis = [0] * 3\n",
        "# y_axis = ['CC', 'MM', 'Adjusted']\n",
        "\n",
        "# for i in range(0, top):\n",
        "#     rank = table_copy.value_counts('rank-' + str(i+1))\n",
        "#     for a in rank.index:\n",
        "#         if a == 0:\n",
        "#             x_axis[0] += rank[a]\n",
        "#         elif a == 2 or a == 3 or a == 4 or a == 5:\n",
        "#             x_axis[1] += rank[a]\n",
        "#         else:\n",
        "#             x_axis[2] += rank[a]\n",
        "\n",
        "# fig, ax = plt.subplots()\n",
        "# ax.bar(y_axis, x_axis, width=1, edgecolor=\"white\", linewidth=0.7)\n",
        "# ax.set(xlim=(0, 9))\n",
        "# plt.show()\n",
        "\n",
        "# print(len(table_copy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# x_axis = [0] * 3\n",
        "# y_axis = ['CC', 'MM', 'Adjusted']\n",
        "# qnt = table['quantifier-ideal-num'].values\n",
        "\n",
        "# for a in qnt:\n",
        "#     if a == 0:\n",
        "#         x_axis[0] += 1\n",
        "#     elif a == 2 or a == 3 or a == 4 or a == 5:\n",
        "#         x_axis[1] += 1\n",
        "#     else:\n",
        "#         x_axis[2] += 1\n",
        "\n",
        "# fig, ax = plt.subplots()\n",
        "# ax.bar(y_axis, x_axis, width=1, edgecolor=\"white\", linewidth=0.7)\n",
        "# ax.set(xlim=(0, 9))\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# i = 0\n",
        "# dataframe = None\n",
        "# X = None\n",
        "# y = None\n",
        "# X_list = []\n",
        "# y_list = []\n",
        "\n",
        "# for f in files:\n",
        "#   if i == 14 or i == 16 or i == 21 or i == 25 or i == 33 or i == 68 or i == 69 or i == 70 or i == 71 or i == 72 or i == 73 or i == 74 or i == 75 or i == 76 or i == 82 or i == 88 or i == 98 or i == 104 or i == 109 or i == 115 or i == 118 or i == 119 or i == 120 or i == 128 or i == 129 or i == 130 or i == 132 or i == 139:\n",
        "#     i += 1\n",
        "#     continue\n",
        "  \n",
        "#   df = pd.read_csv(datasets_path + f)\n",
        "#   df = df.dropna()\n",
        "\n",
        "#   y = df.pop(df.columns[-1])\n",
        "#   X = df\n",
        "\n",
        "#   y_list.append(y.to_numpy())\n",
        "#   X_list.append(X.to_numpy())\n",
        "\n",
        "#   i += 1\n",
        "# i = 0\n",
        "\n",
        "# auc_list = []\n",
        "# clf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
        "# for i in range(0, len(X_list)):\n",
        "#   X = X_list[i]\n",
        "#   y = y_list[i]\n",
        "\n",
        "#   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "#   try:\n",
        "#     clf.fit(X_train, y_train)\n",
        "\n",
        "#     y_prediction_proba = clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "#     # print(metrics.roc_auc_score(y_test, y_prediction_proba))\n",
        "\n",
        "#     auc_list.append(metrics.roc_auc_score(y_test, y_prediction_proba))\n",
        "#   except Exception as e:\n",
        "#     print('Skipping ' + str(i) + '...\\t\\t\\t' + str(e) + '\\n')\n",
        "# # auc_list\n",
        "# auc = pd.DataFrame(data={'auc_score': auc_list})\n",
        "# auc.to_csv('./auc/auc_score.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho_OO9SCg5ie"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# paux = table['quantifier-recommended-num'].T\n",
        "# taux = table['quantifier-ideal-num'].T\n",
        "# paux = paux.values\n",
        "# taux = taux.values\n",
        "\n",
        "# predicted = []\n",
        "# true = []\n",
        "\n",
        "# for i in range(0, len(paux)):\n",
        "#   predicted.append(int(paux[i]))\n",
        "#   true.append(int(taux[i]))\n",
        "\n",
        "# cm = confusion_matrix(true, predicted)\n",
        "# cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu5WViI4vHSf"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=algList)\n",
        "\n",
        "# disp.plot()\n",
        "\n",
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Mq8fYR6NY_7_",
        "txTPtjMCJTyg",
        "eHUI9P42ZC5o"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
