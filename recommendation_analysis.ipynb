{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq8fYR6NY_7_"
      },
      "source": [
        "# Start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmUgYPsj0Tw-",
        "outputId": "ae3e58b5-9821-4a9b-d5d9-a9d6191f532b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from scipy.stats import friedmanchisquare\n",
        "import scikit_posthocs as sp\n",
        "import random\n",
        "import pdb\n",
        "import csv\n",
        "# import pickle as pk\n",
        "\n",
        "experiment_tables_path = './experiment_tables/'\n",
        "experiment_tables_dict = None\n",
        "processed_datasets_df = None\n",
        "\n",
        "datasets_path = './datasets/'\n",
        "files = os.listdir(datasets_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHUI9P42ZC5o"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_remove_dataset_list():\n",
        "    path = './dataset_remove/'\n",
        "    remove_dataset_list = []\n",
        "    temp_remove_dataset_list = pd.read_csv(path + 'remover.csv')\n",
        "    for x in temp_remove_dataset_list.values.tolist():\n",
        "        remove_dataset_list.append(x[0])\n",
        "    \n",
        "    return remove_dataset_list\n",
        "\n",
        "def remove_dataset_from_experiment_tables_dict(experiment_tables_dict, remove_dataset_list):\n",
        "    for dataset in remove_dataset_list:\n",
        "        for key in experiment_tables_dict:\n",
        "            experiment_tables_dict[key] = experiment_tables_dict[key][experiment_tables_dict[key]['dataset_name'] != dataset]\n",
        "            experiment_tables_dict[key].reset_index(drop=True, inplace=True)\n",
        "    return experiment_tables_dict\n",
        "\n",
        "def remove_dataset_from_processed_datasets_df(processed_datasets_df, remove_dataset_list):\n",
        "    for dataset in remove_dataset_list:\n",
        "        processed_datasets_df = processed_datasets_df[processed_datasets_df['dataset'] != dataset + '.csv']\n",
        "    processed_datasets_df.reset_index(drop=True, inplace=True)\n",
        "    return processed_datasets_df\n",
        "\n",
        "def remove_dataset_from_meta_features_table(meta_features_table, remove_dataset_list):\n",
        "    meta_features_table_index = pd.read_csv('./metafeatures/meta-features-table-index.csv')\n",
        "    \n",
        "    for dataset in remove_dataset_list:\n",
        "        remove_index = meta_features_table_index.index[meta_features_table_index['dataset_name'] == dataset+'.csv'].tolist()[0]\n",
        "        meta_features_table = meta_features_table.drop([remove_index])\n",
        "    \n",
        "    meta_features_table.reset_index(drop=True, inplace=True)\n",
        "    return meta_features_table\n",
        "\n",
        "\n",
        "remove_dataset_list = load_remove_dataset_list()\n",
        "\n",
        "\n",
        "# Here we load the Experiment Table from each quantifier\n",
        "# The Experiment Table contains every exectuion + results\n",
        "# from the algorithm.\n",
        "def load_experiment_tables():\n",
        "    exp_tables_dict = {key: None for key in ['CC', 'ACC', 'PACC', 'PCC', 'SMM', 'HDy', 'DyS', 'SORD', 'MS', 'MAX', 'X']}\n",
        "        \n",
        "    for key in exp_tables_dict.keys():\n",
        "        if os.path.isfile(experiment_tables_path + 'experiment_table_' + key + '.csv'):\n",
        "            exp_tables_dict[key] = pd.read_csv(experiment_tables_path + 'experiment_table_' + key + '.csv')\n",
        "        else:\n",
        "            exp_tables_dict[key] = pd.DataFrame(columns=['dataset_name', 'alpha', 'sample_size', 'real_p', 'pred_p', 'abs_error', 'run_time'])\n",
        "    \n",
        "    return exp_tables_dict\n",
        "\n",
        "\n",
        "experiment_tables_dict = load_experiment_tables()\n",
        "experiment_tables_dict = remove_dataset_from_experiment_tables_dict(experiment_tables_dict, remove_dataset_list)\n",
        "\n",
        "df_dict = {key: None for key in list(experiment_tables_dict.keys())}\n",
        "\n",
        "\n",
        "for key in experiment_tables_dict:\n",
        "    df_dict[key] = experiment_tables_dict[key].groupby('dataset_name')['abs_error'].aggregate('mean')\n",
        "\n",
        "\n",
        "meta_features_table = pd.read_csv('./metafeatures/meta-features-table.csv')\n",
        "meta_features_table = remove_dataset_from_meta_features_table(meta_features_table, remove_dataset_list)\n",
        "\n",
        "algList = []\n",
        "tableList = []\n",
        "for counter in df_dict.keys():\n",
        "    algList.append(counter)\n",
        "\n",
        "    y = df_dict[counter].values\n",
        "\n",
        "    X = meta_features_table.values\n",
        "    np.nan_to_num(X, copy=False)\n",
        "    \n",
        "    row, column = np.where(X > np.finfo(np.float32).max)\n",
        "    for i in range(len(row)):\n",
        "        X[row[i]][column[i]] = np.finfo(np.float32).max\n",
        "\n",
        "    tableList.append((X, y))\n",
        "\n",
        "processed_datasets_df = pd.read_csv('./experiment_tables/processed_datasets.csv')\n",
        "processed_datasets_df = remove_dataset_from_processed_datasets_df(processed_datasets_df, remove_dataset_list)\n",
        "\n",
        "datasets = processed_datasets_df['dataset'].tolist()\n",
        "\n",
        "for key in df_dict.keys():\n",
        "    df = pd.DataFrame({'MAE': df_dict[key].to_list(), 'Dataset': datasets})\n",
        "    df.to_csv('./experiment_tables/summarized/experiment_table_' + str(key) + '.csv', index = False)\n",
        "# processed_datasets_df = pd.read_csv('./experiment_tables/processed_datasets.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5826ga0UUBL3"
      },
      "source": [
        "# Train & test recommender with Leave-one-out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code evaluates the recommender (regressor) with Leave-one-out\n",
        "\n",
        "* If this section is commented, just un-comment and run (it's going to take a while)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "BhZgNrPltXXr",
        "outputId": "d2f965f0-c904-4104-e02c-f65bf0392c45"
      },
      "outputs": [],
      "source": [
        "# Regressor ~ Recommender\n",
        "rf_reg = RandomForestRegressor()\n",
        "\n",
        "instance_len = len(tableList[0][0])\n",
        "rf_results = {}\n",
        "\n",
        "# tableList contains the meta-table (X, y) from \n",
        "# each quantifier\n",
        "#\n",
        "# this loop will train and test the RF regressor (recommender)\n",
        "# using the meta-table (X, y) with Leave-One-Out\n",
        "j = 0\n",
        "for (X, y) in tableList:\n",
        "  rf_results_list = []\n",
        "  algName = algList[j]\n",
        "  j += 1\n",
        "\n",
        "  # LEAVE-ONE-OUT\n",
        "  for i in range(0, len(X)):\n",
        "    X_train = np.delete(X, i, 0)\n",
        "    y_train = np.delete(y, i, 0)\n",
        "\n",
        "    X_test = X[i]\n",
        "    X_test = X_test.reshape(1, -1)\n",
        "    y_test = y[i]\n",
        "\n",
        "    rf_reg.fit(X_train, y_train)\n",
        "  \n",
        "    rf_abs_error = rf_reg.predict(X_test)\n",
        "\n",
        "\n",
        "    rf_results_list.append([y_test, rf_abs_error[0]])\n",
        "\n",
        "\n",
        "  rf_results[algName] = rf_results_list\n",
        "\n",
        "\n",
        "# A results table (named 'recommendation table') is constructed\n",
        "# for the regressor (recommender)\n",
        "\n",
        "# # # RANDOM FORESTS\n",
        "# # #\n",
        "data = []\n",
        "cols = []\n",
        "for key in rf_results:\n",
        "  cols.append('abs-error-'+key)\n",
        "  cols.append('abs-error-'+key+'-predicted')\n",
        "cols.append('abs-error-ideal')\n",
        "cols.append('quantifier-ideal')\n",
        "cols.append('quantifier-ideal-num')\n",
        "cols.append('abs-error-recommended')\n",
        "cols.append('quantifier-recommended')\n",
        "cols.append('quantifier-recommended-num')\n",
        "i = 1\n",
        "for key in rf_results:\n",
        "    cols.append('rank-' + str(i))\n",
        "    i += 1\n",
        "\n",
        "\n",
        "i = 0\n",
        "for i in range(0, instance_len):\n",
        "  abs_error_ideal = 2\n",
        "  quantifier_ideal = 'NULL'\n",
        "  quantifier_ideal_num = -1\n",
        "  abs_error_recommended = 2\n",
        "  quantifier_recommended = 'NULL'\n",
        "  quantifier_recommended_num = -1\n",
        "  row = []\n",
        "  algNum = 0\n",
        "  rank = {}\n",
        "\n",
        "  for a in algList:\n",
        "    row.append(rf_results[a][i][0])\n",
        "    row.append(rf_results[a][i][1])\n",
        "\n",
        "    rank[algNum] = rf_results[a][i][1]\n",
        "\n",
        "    if rf_results[a][i][0] < abs_error_ideal:\n",
        "      abs_error_ideal = rf_results[a][i][0]\n",
        "      quantifier_ideal = a\n",
        "      quantifier_ideal_num = algNum\n",
        "\n",
        "    if rf_results[a][i][1] < abs_error_recommended:\n",
        "      abs_error_recommended = rf_results[a][i][1]\n",
        "      quantifier_recommended = a\n",
        "      quantifier_recommended_num = algNum\n",
        "\n",
        "    algNum += 1\n",
        "  rank = sorted(rank.items(), key=lambda item: item[1])\n",
        "\n",
        "  row.append(abs_error_ideal)\n",
        "  row.append(quantifier_ideal)\n",
        "  row.append(quantifier_ideal_num)\n",
        "  row.append(abs_error_recommended)\n",
        "  row.append(quantifier_recommended)\n",
        "  row.append(quantifier_recommended_num)\n",
        "  for key in rank:\n",
        "    row.append(int(key[0]))\n",
        "\n",
        "  data.append(row)\n",
        "\n",
        "rf_table = pd.DataFrame(data, columns = cols)\n",
        "\n",
        "# # This line saves the results of the leaven-one-out evaluation of the recommender:\n",
        "rf_table.to_csv(\"./recommendation/recommendation_table_rf.csv\", index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Recommendation Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We load the Recommendation Table generated in the previous step\n",
        "rf_table = pd.read_csv(\"./recommendation/recommendation_table_rf.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Friedmann Test + Posthoc with <s>Holm Procedure</s> <s>Nemenyi</s> Conover and p-adjust = Holm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code constructs the optimal quantifier sets for each dataset using a non-parametric Friedmann Test + Posthoc with <s>Holm Procedure</s> <s>Nemenyi</s> Conover and p-adjust = Holm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# optimal_dict -> for each dataset (key) there is a set of optimal (adequate) quantifiers (for that problem)\n",
        "# \n",
        "# test_dict -> for each dataset (key) there is a dataframe containing the MAE of each quantifier in that problem\n",
        "# (you can run this code and print 'test_dict' for clarification)\n",
        "test_dict = {}\n",
        "optimal_dict = {}\n",
        "for d in datasets:\n",
        "    test_dict[d.split('.csv')[0]] = None\n",
        "    optimal_dict[d.split('.csv')[0]] = None\n",
        "\n",
        "for dataset in test_dict.keys():\n",
        "    quantifier_error_dict = {key: [] for key in algList}\n",
        "\n",
        "    for alg in algList:\n",
        "        temp_df = experiment_tables_dict[alg].loc[experiment_tables_dict[alg]['dataset_name'] == dataset]\n",
        "        temp_df = temp_df.groupby('alpha').mean(numeric_only = True)\n",
        "        temp_df = temp_df.reset_index()\n",
        "        quantifier_error_dict[alg] = temp_df['abs_error'].tolist()\n",
        "        \n",
        "    test_dict[dataset] = pd.DataFrame()\n",
        "    for key in quantifier_error_dict.keys():\n",
        "        test_dict[dataset][key] = quantifier_error_dict[key]\n",
        "\n",
        "# Here we construct the optimal quantifier set for each dataset\n",
        "#\n",
        "# The best quantifier is selected (lower error) and then every quantifier\n",
        "# that is similar is also included (through a Friedmann Test + Posthoc with Holm Procedure)\n",
        "test_index = 0\n",
        "for dataset in test_dict.keys():\n",
        "    groups = []\n",
        "    for counter in test_dict[dataset].columns.tolist():\n",
        "        groups.append(test_dict[dataset][counter].tolist())\n",
        "    \n",
        "    res = friedmanchisquare(*groups)\n",
        "    if res.pvalue < 0.05:\n",
        "        # HOLM\n",
        "        best = test_dict[dataset].mean().sort_values().index[0]\n",
        "        best_index = test_dict[dataset].columns.tolist().index(best)\n",
        "        opt_set = set()\n",
        "        opt_set.add(best)\n",
        "\n",
        "        # res = sp.posthoc_ttest(groups, p_adjust='holm')\n",
        "        # res = sp.posthoc_nemenyi(groups)\n",
        "        res = sp.posthoc_conover_friedman(np.array(groups).T, p_adjust='holm')\n",
        "    \n",
        "        index = 0\n",
        "        # for element in res[best_index+1]:\n",
        "        for element in res[best_index]: # CONOVER_FRIEDMAN\n",
        "            if element >= 0.05:\n",
        "                opt_set.add(test_dict[dataset].columns.tolist()[index])\n",
        "            index += 1\n",
        "        \n",
        "        optimal_dict[dataset] = opt_set\n",
        "    else:\n",
        "        optimal_dict[dataset] = set(algList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now we save optimal_dict as a single pandas dataframe\n",
        "# This is usefull to plot our \"ASetOpt Histogram\" -> SEE: plots.ipynb\n",
        "columns = ['dataset']\n",
        "columns.extend(algList)\n",
        "optimal_df = pd.DataFrame(columns = columns)\n",
        "\n",
        "for key in optimal_dict.keys():\n",
        "    df_row = [key]\n",
        "    df_row.extend([0] * len(algList))\n",
        "    quantifiers_set = optimal_dict[key]\n",
        "\n",
        "    for quantifier in quantifiers_set:\n",
        "        df_row[algList.index(quantifier) + 1] = 1\n",
        "\n",
        "    optimal_df.loc[len(optimal_df)] = df_row\n",
        "optimal_df.to_csv('./plot_data/optimal_quantifiers.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Topline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code creates the Topline Experiment Table.\n",
        "\n",
        "The Topline is the ideal case, which would happen if we choose the best algorithm for every dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_topline(rf_table, datasets):\n",
        "    topline_experiment_table = pd.DataFrame(columns=['MAE', 'Dataset'])\n",
        "\n",
        "    # Load every dataset (by name)\n",
        "    dt_list = []\n",
        "    for d in datasets:\n",
        "        dt_list.append(d.split('.csv')[0])\n",
        "\n",
        "    # Get all the Ideal Quantifiers from the Recommendation Table\n",
        "    rows = rf_table['quantifier-ideal'].to_list()\n",
        "\n",
        "    # Include in the Experiment Table the MAE of the 'Topline' Quantifier, which would be the best quantifier\n",
        "    # for that specific dataset\n",
        "    i = 0\n",
        "    for r in rows:\n",
        "        topline_experiment_table.loc[len(topline_experiment_table)] = [rf_table.loc[i]['abs-error-' + r], dt_list[i]]\n",
        "        i += 1\n",
        "\n",
        "    # Save the resulting Experiment Table\n",
        "    topline_experiment_table.to_csv('./experiment_tables/summarized/experiment_table_TOPLINE.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Construction of the Top-K sets + weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function creates two dictionaries that are important for the next steps:\n",
        "\n",
        "(1) rf_top_dict:\n",
        "\n",
        "For each dataset (dictonary key) there is a set of the top-K quantifiers for that dataset (dictonary value).\n",
        "This is important to calculate the Hit Rate, by comparing the top-K set with the optimal set.\n",
        "\n",
        "\n",
        "(2) rf_ensemble_top_dict:\n",
        "\n",
        "For each dataset (dictionary key) there is a list containing the Top-K (i) quantifiers and (ii) their respective weights\n",
        "This is important for the Top-K Weighted Ensemble Approach, since we need to know the Top-K quantifiers + their respective\n",
        "weights. For the Top-K Ensemble Approach (not using weights), we only use the Top-K quantifiers (and then we calculate the\n",
        "mean or median of their estimations).\n",
        "\n",
        "You can run this code and print \"rf_top_dict\" and \"rf_ensemble_top_dict\" for better clarification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The \"top\" variable controls the K in the top-K\n",
        "def generate_top_k_dictionary(top, algList, datasets):\n",
        "    dt_list = []\n",
        "    for d in datasets:\n",
        "        dt_list.append(d.split('.csv')[0])\n",
        "\n",
        "    rf_top_dict = {key: None for key in dt_list}\n",
        "\n",
        "    rf_ensemble_top_dict = {}\n",
        "    for key in dt_list:\n",
        "        rf_ensemble_top_dict[key] = []\n",
        "\n",
        "    index = 0\n",
        "    for dataset in rf_top_dict.keys():\n",
        "        quantifier_set = set()\n",
        "        ensemble_quantifier_list = []\n",
        "        ensemble_error_list = []\n",
        "        for i in range(0, top):\n",
        "            # quantifier = random.choice(algList)\n",
        "            quantifier = algList[rf_table.loc[index]['rank-'+str(i+1)]]\n",
        "            quantifier_set.add(quantifier)\n",
        "            ensemble_quantifier_list.append(quantifier)\n",
        "            ensemble_error_list.append(rf_table.loc[index]['abs-error-'+quantifier+'-predicted'])\n",
        "        rf_top_dict[dataset] = quantifier_set\n",
        "        \n",
        "        ensemble_numerator_list = []\n",
        "        denominator = 0\n",
        "        for error in ensemble_error_list:\n",
        "            denominator += 1/error\n",
        "            ensemble_numerator_list.append( 1/error )\n",
        "        \n",
        "        ensemble_weight_list = []\n",
        "        for numerator in ensemble_numerator_list:\n",
        "            ensemble_weight_list.append( numerator / denominator )\n",
        "\n",
        "        rf_ensemble_top_dict[dataset].append(ensemble_quantifier_list)\n",
        "        rf_ensemble_top_dict[dataset].append(ensemble_weight_list)\n",
        "        \n",
        "        index += 1\n",
        "    return rf_top_dict, rf_ensemble_top_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Construction of the Top-K sets + weights (BASELINE)\n",
        "\n",
        "This function is important for constructing the BASELINE Ensemble Method, so we can compare with our Top-K Ensemble Approach.\n",
        "\n",
        "This code generates (1) rf_top_dict and (2) rf_ensemble_top_dict by picking K RANDOM quantifiers (instead of using the recommender)\n",
        "\n",
        "--> Picks K random quantifiers and uses them as an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The \"top\" variable controls the K in the top-K\n",
        "def generate_top_k_dictionary_baseline(top, algList, datasets):\n",
        "    dt_list = []\n",
        "    for d in datasets:\n",
        "        dt_list.append(d.split('.csv')[0])\n",
        "\n",
        "    rf_top_dict = {key: None for key in dt_list}\n",
        "\n",
        "    rf_ensemble_top_dict = {}\n",
        "    for key in dt_list:\n",
        "        rf_ensemble_top_dict[key] = []\n",
        "\n",
        "    index = 0\n",
        "    for dataset in rf_top_dict.keys():\n",
        "        quantifier_set = set()\n",
        "        ensemble_quantifier_list = []\n",
        "        ensemble_error_list = []\n",
        "        alg_list = algList.copy()\n",
        "        for i in range(0, top):\n",
        "            quantifier = random.choice(alg_list)\n",
        "            alg_list.remove(quantifier)\n",
        "\n",
        "            quantifier_set.add(quantifier)\n",
        "            ensemble_quantifier_list.append(quantifier)\n",
        "            ensemble_error_list.append(rf_table.loc[index]['abs-error-'+quantifier+'-predicted'])\n",
        "        rf_top_dict[dataset] = quantifier_set\n",
        "        \n",
        "        ensemble_numerator_list = []\n",
        "        denominator = 0\n",
        "        for error in ensemble_error_list:\n",
        "            denominator += 1/error\n",
        "            ensemble_numerator_list.append( 1/error )\n",
        "        \n",
        "        ensemble_weight_list = []\n",
        "        for numerator in ensemble_numerator_list:\n",
        "            ensemble_weight_list.append( numerator / denominator )\n",
        "\n",
        "        rf_ensemble_top_dict[dataset].append(ensemble_quantifier_list)\n",
        "        rf_ensemble_top_dict[dataset].append(ensemble_weight_list)\n",
        "        \n",
        "        index += 1\n",
        "    return rf_top_dict, rf_ensemble_top_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Top-K Ensemble HitRate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Functions that generate the HitRate for all the quantifiers plus the Top-K ensemble.\n",
        "\n",
        "Then, save it to \"recommender_hit_table\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "hit_rate_table = pd.DataFrame(columns=['Method', 'HitRate', 'Std'])\n",
        "hit_rate_table.to_csv('./recommender_hit_table/recommender_hit_rate_table.csv', index=False)\n",
        "\n",
        "def calculate_quantifiers_hit_rate(algList, datasets):\n",
        "    hit_rate_table = None\n",
        "    if os.path.isfile('./recommender_hit_table/recommender_hit_rate_table.csv'):\n",
        "        hit_rate_table = pd.read_csv('./recommender_hit_table/recommender_hit_rate_table.csv')\n",
        "    else:\n",
        "        hit_rate_table = pd.DataFrame(columns=['Method', 'HitRate', 'Std'])\n",
        "\n",
        "    dt_list = []\n",
        "    for d in datasets:\n",
        "        dt_list.append(d.split('.csv')[0])\n",
        "\n",
        "    for quantifier in algList:\n",
        "        rf_top_hit_list = []\n",
        "        index = 0\n",
        "\n",
        "        A_recommendation_set = set()\n",
        "        A_recommendation_set.add(quantifier)\n",
        "        for dataset in dt_list:\n",
        "            # if A_recommendation_set INTERSECTION A_optimal_set is not NULL, then a Hit is attributed\n",
        "            if len(A_recommendation_set.intersection(optimal_dict[dataset])) != 0:\n",
        "                rf_top_hit_list.append(1)\n",
        "            else:\n",
        "                rf_top_hit_list.append(0)\n",
        "            index += 1\n",
        "\n",
        "        rf_top_hit_rate = np.mean(rf_top_hit_list)\n",
        "        rf_top_std = np.std(rf_top_hit_list)\n",
        "        hit_rate_table.loc[len(hit_rate_table)] = [quantifier, rf_top_hit_rate, rf_top_std]\n",
        "\n",
        "    # Save the HitRate to the Recommender Hit Table\n",
        "    hit_rate_table.to_csv('./recommender_hit_table/recommender_hit_rate_table.csv', index=False)\n",
        "\n",
        "\n",
        "\n",
        "def calculate_ensemble_hit_rate(rf_top_dict, datasets, name):\n",
        "    hit_rate_table = None\n",
        "    if os.path.isfile('./recommender_hit_table/recommender_hit_rate_table.csv'):\n",
        "        hit_rate_table = pd.read_csv('./recommender_hit_table/recommender_hit_rate_table.csv')\n",
        "    else:\n",
        "        hit_rate_table = pd.DataFrame(columns=['Method', 'HitRate', 'Std'])\n",
        "\n",
        "\n",
        "    dt_list = []\n",
        "    for d in datasets:\n",
        "        dt_list.append(d.split('.csv')[0])\n",
        "\n",
        "    rf_top_hit_list = []\n",
        "    index = 0\n",
        "    for dataset in dt_list:\n",
        "        # if A_recommendation_set INTERSECTION A_optimal_set is not NULL, then a Hit is attributed\n",
        "        A_recommendation_set = rf_top_dict[dataset]\n",
        "        if len(A_recommendation_set.intersection(optimal_dict[dataset])) != 0:\n",
        "            rf_top_hit_list.append(1)\n",
        "        else:\n",
        "            rf_top_hit_list.append(0)\n",
        "        index += 1\n",
        "\n",
        "    rf_top_hit_rate = np.mean(rf_top_hit_list)\n",
        "    rf_top_std = np.std(rf_top_hit_list)\n",
        "\n",
        "\n",
        "    # Save the HitRate to the Recommender Hit Table\n",
        "    # hit_rate_table = pd.read_csv('./recommender_hit_table/recommender_hit_rate_table.csv')\n",
        "    hit_rate_table.loc[len(hit_rate_table)] = [name, rf_top_hit_rate, rf_top_std]\n",
        "    hit_rate_table.to_csv('./recommender_hit_table/recommender_hit_rate_table.csv', index=False)\n",
        "\n",
        "\n",
        "\n",
        "def calculate_ensemble_hit_rate_BASELINE(rf_top_dict, dt_list):\n",
        "    rf_top_hit_list = []\n",
        "    index = 0\n",
        "    for dataset in dt_list:\n",
        "        # if A_recommendation_set INTERSECTION A_optimal_set is not NULL, then a Hit is attributed\n",
        "        A_recommendation_set = rf_top_dict[dataset]\n",
        "        if len(A_recommendation_set.intersection(optimal_dict[dataset])) != 0:\n",
        "            rf_top_hit_list.append(1)\n",
        "        else:\n",
        "            rf_top_hit_list.append(0)\n",
        "        index += 1\n",
        "\n",
        "    rf_top_hit_rate = np.mean(rf_top_hit_list)\n",
        "    rf_top_std = np.std(rf_top_hit_list)\n",
        "\n",
        "    return rf_top_hit_rate, rf_top_std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Top-K Ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function constructs the top-K ensemble method\n",
        "\n",
        "Each recommender predicts a MAE for their specific quantifier\n",
        "\n",
        "Quantifiers are then ordered by their MAE (crescent order)\n",
        "\n",
        "The Top-k are selected and are used to give their own estimates of the class prevalence {p_1, ..., p_k}\n",
        "\n",
        "The \"final\" prevalence is the mean OR median (we use both approaches) of the k estimations\n",
        "\n",
        "Then, the absolute error of our approach (mean or median) is calculated -> ABSOLUTE(real_prevalence - estimated_prevalence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def construct_top_k_ensemble(rf_top_dict, top, name):\n",
        "    top_k_experiment_table_mean = pd.DataFrame(columns=['MAE', 'Dataset'])\n",
        "    top_k_experiment_table_median = pd.DataFrame(columns=['MAE', 'Dataset'])\n",
        "\n",
        "    dt_list = []\n",
        "    for d in datasets:\n",
        "        dt_list.append(d.split('.csv')[0])\n",
        "\n",
        "    index = 0\n",
        "    for item in rf_top_dict.items():\n",
        "        quantifier_set = item[1]\n",
        "        \n",
        "        pred_p_mean = [0] * 20\n",
        "        temp_pred_p_median = [ [] for _ in range(20) ]\n",
        "        pred_p_median = []\n",
        "        real_p = None\n",
        "        for x in quantifier_set:\n",
        "            # Get the real_p (prevalence) of the dataset and the predicted_p (prevalence) by the quantifier\n",
        "            real_p = experiment_tables_dict[x].loc[experiment_tables_dict[x]['dataset_name'] == dt_list[index]].groupby('alpha').mean(numeric_only=True)['real_p'].values.tolist()\n",
        "            pred_p = experiment_tables_dict[x].loc[experiment_tables_dict[x]['dataset_name'] == dt_list[index]].groupby('alpha').mean(numeric_only=True)['pred_p'].values.tolist()\n",
        "            for i in range(0, len(real_p)):\n",
        "                pred_p_mean[i] += pred_p[i]\n",
        "                temp_pred_p_median[i].append(pred_p[i])\n",
        "        \n",
        "        # The \"final\" prevalence will be the mean or median of each estimated prevalence by each quantifier\n",
        "        for i in range(0, len(pred_p_mean)):\n",
        "            pred_p_mean[i] /= len(quantifier_set)\n",
        "            pred_p_median.append(np.median(temp_pred_p_median[i]))\n",
        "        \n",
        "        abs_error_mean_list = []\n",
        "        abs_error_median_list = []\n",
        "        for i in range(0, len(pred_p_mean)):\n",
        "            abs_error_mean_list.append(abs(real_p[i] - pred_p_mean[i]))\n",
        "            abs_error_median_list.append(abs(real_p[i] - pred_p_median[i]))\n",
        "        abs_error_mean = np.mean(abs_error_mean_list)\n",
        "        abs_error_median = np.mean(abs_error_median_list)\n",
        "\n",
        "        top_k_experiment_table_mean.loc[len(top_k_experiment_table_mean)] = [abs_error_mean, dt_list[index]]\n",
        "        top_k_experiment_table_median.loc[len(top_k_experiment_table_median)] = [abs_error_median, dt_list[index]]\n",
        "        # experiment_table.loc[len(experiment_table)] = ['Random Forests+Top'+str(top)+' (MEAN)', dt_list[index], abs_error_mean]\n",
        "        # experiment_table.loc[len(experiment_table)] = ['Random Forests+Top'+str(top)+' (MEDIAN)', dt_list[index], abs_error_median]\n",
        "\n",
        "        index += 1\n",
        "\n",
        "\n",
        "    if top == 1:\n",
        "        top_k_experiment_table_mean.to_csv('./experiment_tables/summarized/'+name+'.csv', index=False)\n",
        "    else:\n",
        "        top_k_experiment_table_mean.to_csv('./experiment_tables/summarized/'+name+'_MEAN.csv', index=False)\n",
        "        top_k_experiment_table_median.to_csv('./experiment_tables/summarized/'+name+'_MEDIAN.csv', index=False)\n",
        "    \n",
        "    return top_k_experiment_table_mean, top_k_experiment_table_median"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Top-K Ensemble Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def construct_top_k_ensemble_BASELINE(top, repeat, algList, datasets, name):\n",
        "    top_k_experiment_mean_dict = {}\n",
        "    top_k_experiment_median_dict = {}\n",
        "\n",
        "    top_k_hit_rate_list = []\n",
        "    top_k_hit_rate_std_list = []\n",
        "\n",
        "    dt_list = []\n",
        "    for d in datasets:\n",
        "        dt_list.append(d.split('.csv')[0])\n",
        "        top_k_experiment_mean_dict[d.split('.csv')[0]] = []\n",
        "        top_k_experiment_median_dict[d.split('.csv')[0]] = []\n",
        "    \n",
        "    for repeat_index in range(0, repeat):\n",
        "        rf_top_dict, _ = generate_top_k_dictionary_baseline(top, algList, datasets)\n",
        "\n",
        "        rf_top_hit_rate, rf_top_std = calculate_ensemble_hit_rate_BASELINE(rf_top_dict, dt_list)\n",
        "        top_k_hit_rate_list.append(rf_top_hit_rate)\n",
        "        top_k_hit_rate_std_list.append(rf_top_std)\n",
        "        \n",
        "        index = 0\n",
        "        for item in rf_top_dict.items():\n",
        "            quantifier_set = item[1]\n",
        "            \n",
        "            pred_p_mean = [0] * 20\n",
        "            temp_pred_p_median = [ [] for _ in range(20) ]\n",
        "            pred_p_median = []\n",
        "            real_p = None\n",
        "            for x in quantifier_set:\n",
        "                # Get the real_p (prevalence) of the dataset and the predicted_p (prevalence) by the quantifier\n",
        "                real_p = experiment_tables_dict[x].loc[experiment_tables_dict[x]['dataset_name'] == dt_list[index]].groupby('alpha').mean(numeric_only=True)['real_p'].values.tolist()\n",
        "                pred_p = experiment_tables_dict[x].loc[experiment_tables_dict[x]['dataset_name'] == dt_list[index]].groupby('alpha').mean(numeric_only=True)['pred_p'].values.tolist()\n",
        "                for i in range(0, len(real_p)):\n",
        "                    pred_p_mean[i] += pred_p[i]\n",
        "                    temp_pred_p_median[i].append(pred_p[i])\n",
        "            \n",
        "            # The \"final\" prevalence will be the mean or median of each estimated prevalence by each quantifier\n",
        "            for i in range(0, len(pred_p_mean)):\n",
        "                pred_p_mean[i] /= len(quantifier_set)\n",
        "                pred_p_median.append(np.median(temp_pred_p_median[i]))\n",
        "            \n",
        "            abs_error_mean_list = []\n",
        "            abs_error_median_list = []\n",
        "            for i in range(0, len(pred_p_mean)):\n",
        "                abs_error_mean_list.append(abs(real_p[i] - pred_p_mean[i]))\n",
        "                abs_error_median_list.append(abs(real_p[i] - pred_p_median[i]))\n",
        "            abs_error_mean = np.mean(abs_error_mean_list)\n",
        "            abs_error_median = np.mean(abs_error_median_list)\n",
        "\n",
        "            # top_k_experiment_table_mean.loc[len(top_k_experiment_table_mean)] = [abs_error_mean, dt_list[index]]\n",
        "            # top_k_experiment_table_median.loc[len(top_k_experiment_table_median)] = [abs_error_median, dt_list[index]]\n",
        "\n",
        "            top_k_experiment_mean_dict[dt_list[index]].append(abs_error_mean)\n",
        "            top_k_experiment_median_dict[dt_list[index]].append(abs_error_median)\n",
        "\n",
        "            index += 1\n",
        "\n",
        "\n",
        "    top_k_experiment_table_mean = pd.DataFrame(columns=['MAE', 'Dataset'])\n",
        "    top_k_experiment_table_median = pd.DataFrame(columns=['MAE', 'Dataset'])\n",
        "    for dt in dt_list:\n",
        "        abs_error_mean = np.mean(top_k_experiment_mean_dict[dt])\n",
        "        abs_error_median = np.mean(top_k_experiment_median_dict[dt])\n",
        "\n",
        "        # pdb.set_trace()\n",
        "\n",
        "        top_k_experiment_table_mean.loc[len(top_k_experiment_table_mean)] = [abs_error_mean, dt]\n",
        "        top_k_experiment_table_median.loc[len(top_k_experiment_table_median)] = [abs_error_median, dt]\n",
        "\n",
        "    top_k_hit_rate = np.mean(top_k_hit_rate_list)\n",
        "    top_k_hit_rate_std = np.mean(top_k_hit_rate_std_list)\n",
        "\n",
        "    if top == 1:\n",
        "        top_k_experiment_table_mean.to_csv('./experiment_tables/summarized/'+name+'.csv', index=False)\n",
        "    else:\n",
        "        top_k_experiment_table_mean.to_csv('./experiment_tables/summarized/'+name+'_MEAN.csv', index=False)\n",
        "        top_k_experiment_table_median.to_csv('./experiment_tables/summarized/'+name+'_MEDIAN.csv', index=False)\n",
        "    \n",
        "    return top_k_experiment_table_mean, top_k_experiment_table_median"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Top-K Weighted Ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function implements the weighted top-K ensemble method\n",
        "\n",
        "Each recommender predicts a MAE for their specific quantifier\n",
        "\n",
        "Quantifiers are then ordered by their MAE (crescent order)\n",
        "\n",
        "A weight is attributed to each quantifier based on their MAE. The lower the MAE, the bigger the weight. The first quantifier (Top-1) has the biggest weight, the second (Top-2) has the seccond biggest weight, and so on.\n",
        "\n",
        "ALL of the weights sum up to one -> weight_quantifier_1 + weight_quantifier_2 + ... + weight_quantifier_k = 1\n",
        "\n",
        "The Top-k are selected and are used to give it's own estimate of the class prevalence {p_1, ..., p_k}\n",
        "\n",
        "The \"final\" prevalence is the sum of the k estimations multiplied by their respective weight:\n",
        "\n",
        "prevalence_estimation = weight_quantifier_1 * prevalence_quantifier_1 + ... + weight_quantifier_k * prevalence_quantifier_k\n",
        "\n",
        "Then, the absolute error of our approach is calculated -> ABSOLUTE(real_prevalence - estimated_prevalence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def construct_top_k_weighted_ensemble(rf_ensemble_top_dict, top, name):\n",
        "    top_k_weighted_experiment_table = pd.DataFrame(columns=['MAE', 'Dataset'])\n",
        "\n",
        "    dt_list = []\n",
        "    for d in datasets:\n",
        "        dt_list.append(d.split('.csv')[0])\n",
        "\n",
        "    index = 0\n",
        "    for item in rf_ensemble_top_dict.items():\n",
        "        quantifier_list = item[1][0]\n",
        "        weight_list = item[1][1]\n",
        "        prevalence_list = [0] * 20\n",
        "\n",
        "        real_p = None\n",
        "        for x in quantifier_list:\n",
        "            # Get the real_p (prevalence) of the dataset and the predicted_p (prevalence) by the quantifier\n",
        "            real_p = experiment_tables_dict[x].loc[experiment_tables_dict[x]['dataset_name'] == dt_list[index]].groupby('alpha').mean(numeric_only=True)['real_p'].values.tolist()\n",
        "            pred_p = experiment_tables_dict[x].loc[experiment_tables_dict[x]['dataset_name'] == dt_list[index]].groupby('alpha').mean(numeric_only=True)['pred_p'].values.tolist()\n",
        "            \n",
        "            # The \"final\" prevalence will be the prevalences of the K quantifiers adjusted by their weights.\n",
        "            # Note that pred_p has 20 prevalences, since we test a quantifier on a dataset 20 times (varying the class distribution).\n",
        "            for i in range(0, len(pred_p)):\n",
        "                prevalence_list[i] += pred_p[i] * weight_list[quantifier_list.index(x)]\n",
        "        \n",
        "        ensemble_abs_error_list = []\n",
        "        for i in range(0, len(prevalence_list)):\n",
        "            ensemble_abs_error_list.append( abs( real_p[i] - prevalence_list[i] ) )\n",
        "\n",
        "        ensemble_abs_error = np.mean(ensemble_abs_error_list)\n",
        "        \n",
        "        top_k_weighted_experiment_table.loc[len(top_k_weighted_experiment_table)] = [ensemble_abs_error, dt_list[index]]\n",
        "        # experiment_table.loc[len(experiment_table)] = ['Random Forests+Top'+str(top)+'+Weighted (ENSEMBLE)', dt_list[index], ensemble_abs_error]\n",
        "        index += 1\n",
        "\n",
        "\n",
        "    if top != 1: \n",
        "        top_k_weighted_experiment_table.to_csv('./experiment_tables/summarized/'+name+'_WEIGHTED.csv', index=False)\n",
        "\n",
        "    return top_k_weighted_experiment_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we run all the functions above to generate the results for the following:\n",
        "* Topline\n",
        "* Baseline Ensemble Top 3\n",
        "* Baseline Ensemble Top 3 + Weighted\n",
        "* Baseline Ensemble Top 5\n",
        "* Baseline Ensemble Top 5 + Weighted\n",
        "* Recommender Top 1\n",
        "* Recommender Top 3\n",
        "* Recommender Top 3 + Weighted\n",
        "* Recommender Top 5\n",
        "* Recommender Top 5 + Weighted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "############################################################### TOPLINE\n",
        "generate_topline(rf_table, datasets)\n",
        "# PS: TOPLINE HIT RATE IS OBVIOUSLY 100%\n",
        "\n",
        "\n",
        "############################################################### BASELINE TOP 1\n",
        "top = 1\n",
        "construct_top_k_ensemble_BASELINE(top, 30, algList, datasets, 'experiment_table_BASELINE_TOP1')\n",
        "\n",
        "\n",
        "############################################################### BASELINE TOP 3\n",
        "top = 3\n",
        "construct_top_k_ensemble_BASELINE(top, 30, algList, datasets, 'experiment_table_BASELINE_TOP3')\n",
        "\n",
        "\n",
        "############################################################### BASELINE TOP 5\n",
        "top = 5\n",
        "construct_top_k_ensemble_BASELINE(top, 30, algList, datasets, 'experiment_table_BASELINE_TOP5')\n",
        "\n",
        "\n",
        "############################################################### TOP 1\n",
        "top = 1\n",
        "\n",
        "# We set rf_top_dict AND rf_ensemble_top_weighted_dict with\n",
        "# the function that uses our recommendation (OUR APPROACH)\n",
        "rf_top_dict, rf_ensemble_top_weighted_dict = generate_top_k_dictionary(top, algList, datasets)\n",
        "\n",
        "# Then we construct the ensembles with them\n",
        "construct_top_k_ensemble(rf_top_dict, top, 'experiment_table_TOP1')\n",
        "construct_top_k_weighted_ensemble(rf_ensemble_top_weighted_dict, top, 'experiment_table_TOP1')\n",
        "calculate_ensemble_hit_rate(rf_top_dict, datasets, 'Top1')\n",
        "\n",
        "\n",
        "############################################################### TOP 3\n",
        "top = 3\n",
        "\n",
        "# We set rf_top_dict AND rf_ensemble_top_weighted_dict with\n",
        "# the function that uses our recommendation (OUR APPROACH)\n",
        "rf_top_dict, rf_ensemble_top_weighted_dict = generate_top_k_dictionary(top, algList, datasets)\n",
        "\n",
        "# Then we construct the ensembles with them\n",
        "construct_top_k_ensemble(rf_top_dict, top, 'experiment_table_TOP3')\n",
        "construct_top_k_weighted_ensemble(rf_ensemble_top_weighted_dict, top, 'experiment_table_TOP3')\n",
        "calculate_ensemble_hit_rate(rf_top_dict, datasets, 'Top3')\n",
        "\n",
        "\n",
        "############################################################### TOP 5\n",
        "top = 5\n",
        "\n",
        "# We set rf_top_dict AND rf_ensemble_top_weighted_dict with the function that uses our recommendation (OUR APPROACH)\n",
        "rf_top_dict, rf_ensemble_top_weighted_dict = generate_top_k_dictionary(top, algList, datasets)\n",
        "\n",
        "# Then we construct the ensembles with them\n",
        "construct_top_k_ensemble(rf_top_dict, top, 'experiment_table_TOP5')\n",
        "construct_top_k_weighted_ensemble(rf_ensemble_top_weighted_dict, top, 'experiment_table_TOP5')\n",
        "calculate_ensemble_hit_rate(rf_top_dict, datasets, 'Top5')\n",
        "\n",
        "\n",
        "################################################################ BASE QUANTIFIERS HIT RATE\n",
        "calculate_quantifiers_hit_rate(algList, datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unify all the Experiment Tables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the previous steps, we created Experiment Tables for each quantifier. The experiment tables for our Top-K approach were created here, while the experiment tables for the other quantifiers were created in the APP function and were summarized here (refer to 'summarized' folder).\n",
        "\n",
        "Now we are going to unify all these experiment tables into a single Experiment Table (even more summarized) consisting of a DataFrame containing (1) the method (algorithm), (2) the Dataset name and (3) MAE.\n",
        "\n",
        "This is just to facilitate our analysis and to condense all the results to a single file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The 'even more summarized' Experiment Table containts only the Method, Dataset and MAE\n",
        "even_more_summarized_experiment_table = pd.DataFrame(columns=['Method', 'Dataset', 'MAE'])\n",
        "\n",
        "dt_list = []\n",
        "for d in datasets:\n",
        "    dt_list.append(d.split('.csv')[0])\n",
        "\n",
        "for filename in os.listdir('./experiment_tables/summarized/'):\n",
        "    if(filename == 'BKP'):\n",
        "        continue\n",
        "\n",
        "    # We load each experiment table\n",
        "    df = pd.read_csv('./experiment_tables/summarized/'+filename)\n",
        "    \n",
        "    method_name = filename.split('experiment_table_')[1].split('.csv')[0]\n",
        "    method_name = method_name.replace('_', '+')\n",
        "    \n",
        "    # And then we include each row in the 'even more summarized' one\n",
        "    index = 0\n",
        "    for row in df.iterrows():\n",
        "        even_more_summarized_experiment_table.loc[len(even_more_summarized_experiment_table)] = [method_name, dt_list[index], row[1]['MAE']]\n",
        "        index += 1\n",
        "\n",
        "# Save the resulting 'even more summarized' experiment table\n",
        "even_more_summarized_experiment_table.to_csv('./experiment_tables/even_more_summarized/experiment_table.csv', index=False)\n",
        "\n",
        "# Save info to plot data folder\n",
        "even_more_summarized_experiment_table.to_csv('./plot_data/plot_tables/experiment_table.csv', index=False)\n",
        "hit_rate_table = pd.read_csv('./recommender_hit_table/recommender_hit_rate_table.csv')\n",
        "hit_rate_table.to_csv('./plot_data/plot_tables/recommender_hit_rate_table.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Mq8fYR6NY_7_",
        "txTPtjMCJTyg",
        "eHUI9P42ZC5o"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
