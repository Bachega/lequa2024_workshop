{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq8fYR6NY_7_"
      },
      "source": [
        "# Start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmUgYPsj0Tw-",
        "outputId": "ae3e58b5-9821-4a9b-d5d9-a9d6191f532b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from scipy.stats import friedmanchisquare\n",
        "from scikit_posthocs import posthoc_ttest\n",
        "\n",
        "experiment_tables_path = './experiment_tables/'\n",
        "experiment_tables_dict = None\n",
        "processed_datasets_df = None\n",
        "\n",
        "datasets_path = './datasets/'\n",
        "files = os.listdir(datasets_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHUI9P42ZC5o"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here we load the Experiment Table from each quantifier\n",
        "# The Experiment Table contains every exectuion + results\n",
        "# from the algorithm.\n",
        "def load_experiment_tables():\n",
        "    exp_tables_dict = {key: None for key in ['CC', 'ACC', 'PACC', 'PCC', 'SMM', 'HDy', 'DyS', 'SORD', 'MS', 'MS2', 'MAX', 'X']}\n",
        "        \n",
        "    for key in exp_tables_dict.keys():\n",
        "        if os.path.isfile(experiment_tables_path + 'experiment_table_' + key + '.csv'):\n",
        "            exp_tables_dict[key] = pd.read_csv(experiment_tables_path + 'experiment_table_' + key + '.csv')\n",
        "        else:\n",
        "            exp_tables_dict[key] = pd.DataFrame(columns=['dataset_name', 'alpha', 'sample_size', 'real_p', 'pred_p', 'abs_error', 'run_time'])\n",
        "    \n",
        "    return exp_tables_dict\n",
        "\n",
        "experiment_tables_dict = load_experiment_tables()\n",
        "df_dict = {key: None for key in list(experiment_tables_dict.keys())}\n",
        "\n",
        "\n",
        "for key in experiment_tables_dict:\n",
        "    df_dict[key] = experiment_tables_dict[key].groupby('dataset_name')['abs_error'].aggregate('mean')\n",
        "\n",
        "meta_features_table = pd.read_csv('./metafeatures/meta-features-table.csv')\n",
        "\n",
        "algList = []\n",
        "tableList = []\n",
        "for counter in df_dict.keys():\n",
        "    algList.append(counter)\n",
        "\n",
        "    y = df_dict[counter].values\n",
        "\n",
        "    X = meta_features_table.values\n",
        "    np.nan_to_num(X, copy=False)\n",
        "    \n",
        "    row, column = np.where(X > np.finfo(np.float32).max)\n",
        "    for i in range(len(row)):\n",
        "        X[row[i]][column[i]] = np.finfo(np.float32).max\n",
        "\n",
        "    tableList.append((X, y))\n",
        "\n",
        "processed_datasets_df = pd.read_csv('./experiment_tables/processed_datasets.csv')\n",
        "datasets = processed_datasets_df['dataset'].tolist()\n",
        "\n",
        "for key in df_dict.keys():\n",
        "    df = pd.DataFrame({'MAE': df_dict[key].to_list(), 'Dataset': datasets})\n",
        "    df.to_csv('./experiment_tables/summarized/experiment_table_' + str(key) + '.csv', index = False)\n",
        "processed_datasets_df = pd.read_csv('./experiment_tables/processed_datasets.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5826ga0UUBL3"
      },
      "source": [
        "# Train & test recommender with Leave-one-out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code evaluates the recommender (regressor) with Leave-one-out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "BhZgNrPltXXr",
        "outputId": "d2f965f0-c904-4104-e02c-f65bf0392c45"
      },
      "outputs": [],
      "source": [
        "# # Regressor ~ Recommender\n",
        "# rf_reg = RandomForestRegressor()\n",
        "\n",
        "# instance_len = len(tableList[0][0])\n",
        "# rf_results = {}\n",
        "\n",
        "# # tableList contains the meta-table (X, y) from \n",
        "# # each quantifier\n",
        "# #\n",
        "# # this loop will train and test the RF regressor (recommender)\n",
        "# # using the meta-table (X, y) with Leave-One-Out\n",
        "# j = 0\n",
        "# for (X, y) in tableList:\n",
        "#   rf_results_list = []\n",
        "#   algName = algList[j]\n",
        "#   j += 1\n",
        "\n",
        "#   # LEAVE-ONE-OUT\n",
        "#   for i in range(0, len(X)):\n",
        "#     X_train = np.delete(X, i, 0)\n",
        "#     y_train = np.delete(y, i, 0)\n",
        "\n",
        "#     X_test = X[i]\n",
        "#     X_test = X_test.reshape(1, -1)\n",
        "#     y_test = y[i]\n",
        "\n",
        "#     rf_reg.fit(X_train, y_train)\n",
        "  \n",
        "#     rf_abs_error = rf_reg.predict(X_test)\n",
        "\n",
        "\n",
        "#     rf_results_list.append([y_test, rf_abs_error[0]])\n",
        "\n",
        "\n",
        "#   rf_results[algName] = rf_results_list\n",
        "\n",
        "\n",
        "# # A results table (named 'recommendation table') is constructed\n",
        "# # for the regressor (recommender)\n",
        "\n",
        "# # # # RANDOM FORESTS\n",
        "# # # #\n",
        "# data = []\n",
        "# cols = []\n",
        "# for key in rf_results:\n",
        "#   cols.append('abs-error-'+key)\n",
        "#   cols.append('abs-error-'+key+'-predicted')\n",
        "# cols.append('abs-error-ideal')\n",
        "# cols.append('quantifier-ideal')\n",
        "# cols.append('quantifier-ideal-num')\n",
        "# cols.append('abs-error-recommended')\n",
        "# cols.append('quantifier-recommended')\n",
        "# cols.append('quantifier-recommended-num')\n",
        "# i = 1\n",
        "# for key in rf_results:\n",
        "#     cols.append('rank-' + str(i))\n",
        "#     i += 1\n",
        "\n",
        "\n",
        "# i = 0\n",
        "# for i in range(0, instance_len):\n",
        "#   abs_error_ideal = 2\n",
        "#   quantifier_ideal = 'NULL'\n",
        "#   quantifier_ideal_num = -1\n",
        "#   abs_error_recommended = 2\n",
        "#   quantifier_recommended = 'NULL'\n",
        "#   quantifier_recommended_num = -1\n",
        "#   row = []\n",
        "#   algNum = 0\n",
        "#   rank = {}\n",
        "\n",
        "#   for a in algList:\n",
        "#     row.append(rf_results[a][i][0])\n",
        "#     row.append(rf_results[a][i][1])\n",
        "\n",
        "#     rank[algNum] = rf_results[a][i][1]\n",
        "\n",
        "#     if rf_results[a][i][0] < abs_error_ideal:\n",
        "#       abs_error_ideal = rf_results[a][i][0]\n",
        "#       quantifier_ideal = a\n",
        "#       quantifier_ideal_num = algNum\n",
        "\n",
        "#     if rf_results[a][i][1] < abs_error_recommended:\n",
        "#       abs_error_recommended = rf_results[a][i][1]\n",
        "#       quantifier_recommended = a\n",
        "#       quantifier_recommended_num = algNum\n",
        "\n",
        "#     algNum += 1\n",
        "#   rank = sorted(rank.items(), key=lambda item: item[1])\n",
        "\n",
        "#   row.append(abs_error_ideal)\n",
        "#   row.append(quantifier_ideal)\n",
        "#   row.append(quantifier_ideal_num)\n",
        "#   row.append(abs_error_recommended)\n",
        "#   row.append(quantifier_recommended)\n",
        "#   row.append(quantifier_recommended_num)\n",
        "#   for key in rank:\n",
        "#     row.append(int(key[0]))\n",
        "\n",
        "#   data.append(row)\n",
        "\n",
        "# rf_table = pd.DataFrame(data, columns = cols)\n",
        "\n",
        "# # # This line saves the results of the leaven-one-out evaluation of the recommender:\n",
        "# # rf_table.to_csv(\"./recommendation/recommendation_table_rf.csv\", index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Recommendation Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We load the Recommendation Table generated in the previous step\n",
        "rf_table = pd.read_csv(\"./recommendation/recommendation_table_rf.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Friedmann Test + Posthoc with Holm Procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code constructs the optimal quantifier sets for each dataset using a non-parametric Friedmann Test + Posthoc with Holm Procedure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# optimal_dict -> for each dataset (key) there is a set of optimal (adequate) quantifiers (for that problem)\n",
        "# \n",
        "# test_dict -> for each dataset (key) there is a dataframe containing the MAE of each quantifier in that problem\n",
        "# (you can run this code and print 'test_dict' for clarification)\n",
        "test_dict = {}\n",
        "optimal_dict = {}\n",
        "for d in datasets:\n",
        "    test_dict[d.split('.csv')[0]] = None\n",
        "    optimal_dict[d.split('.csv')[0]] = None\n",
        "\n",
        "for dataset in test_dict.keys():\n",
        "    quantifier_error_dict = {key: [] for key in algList}\n",
        "\n",
        "    for alg in algList:\n",
        "        temp_df = experiment_tables_dict[alg].loc[experiment_tables_dict[alg]['dataset_name'] == dataset]\n",
        "        temp_df = temp_df.groupby('alpha').mean(numeric_only = True)\n",
        "        temp_df = temp_df.reset_index()\n",
        "        quantifier_error_dict[alg] = temp_df['abs_error'].tolist()\n",
        "        \n",
        "    test_dict[dataset] = pd.DataFrame()\n",
        "    for key in quantifier_error_dict.keys():\n",
        "        test_dict[dataset][key] = quantifier_error_dict[key]\n",
        "\n",
        "# Here we construct the optimal quantifier set for each dataset\n",
        "#\n",
        "# The best quantifier is selected (lower error) and then every quantifier\n",
        "# that is similar is also included (through a Friedmann Test + Posthoc with Holm Procedure)\n",
        "test_index = 0\n",
        "for dataset in test_dict.keys():\n",
        "    groups = []\n",
        "    for counter in test_dict[dataset].columns.tolist():\n",
        "        groups.append(test_dict[dataset][counter].tolist())\n",
        "    res = friedmanchisquare(*groups)\n",
        "    if res.pvalue < 0.05:\n",
        "        # HOLM\n",
        "        best = test_dict[dataset].mean().sort_values().index[0]\n",
        "        best_index = test_dict[dataset].columns.tolist().index(best)\n",
        "        opt_set = set()\n",
        "        opt_set.add(best)\n",
        "\n",
        "        res = posthoc_ttest(groups, p_adjust='holm')\n",
        "        \n",
        "        index = 0\n",
        "        for element in res[best_index+1]:\n",
        "            if element >= 0.05:\n",
        "                opt_set.add(test_dict[dataset].columns.tolist()[index])\n",
        "            index += 1\n",
        "        \n",
        "        optimal_dict[dataset] = opt_set\n",
        "    else:\n",
        "        optimal_dict[dataset] = set(algList)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Topline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code creates the Topline Experiment Table.\n",
        "\n",
        "The Topline is the ideal case, which would happen if we choose the best algorithm for every dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "topline_experiment_table = pd.DataFrame(columns=['MAE', 'Dataset'])\n",
        "\n",
        "# Load every dataset (by name)\n",
        "dt_list = []\n",
        "for d in datasets:\n",
        "    dt_list.append(d.split('.csv')[0])\n",
        "\n",
        "# Get all the Ideal Quantifiers from the Recommendation Table\n",
        "rows = rf_table['quantifier-ideal'].to_list()\n",
        "topline_error_list = []\n",
        "\n",
        "# Include in the Experiment Table the MAE of the 'Topline' Quantifier, which would be the best quantifier\n",
        "# for that specific dataset\n",
        "i = 0\n",
        "for r in rows:\n",
        "    topline_experiment_table.loc[len(topline_experiment_table)] = [rf_table.loc[i]['abs-error-' + r], dt_list[i]]\n",
        "    i += 1\n",
        "\n",
        "# Save the resulting Experiment Table\n",
        "topline_experiment_table.to_csv('./experiment_tables/summarized/experiment_table_TOPLINE.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Construction of the Top-K sets + weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code creates two dictionaries that are important for the next steps:\n",
        "\n",
        "(1) rf_top_dict:\n",
        "\n",
        "      For each dataset (dictonary key) there is a set of the top-K quantifiers for that dataset (dictonary value).\n",
        "      This is important to calculate the Hit Rate, by comparing the top-K set with the optimal set.\n",
        "\n",
        "(2) rf_ensemble_top_dict:\n",
        "\n",
        "      For each dataset (dictionary key) there is a list containing the Top-K (i) quantifiers and (ii) their respective weights\n",
        "      This is important for the Top-K Weighted Ensemble Approach, since we need to know the Top-K quantifiers + their respective\n",
        "      weights. For the Top-K Ensemble Approach (not using weights), we only use the Top-K quantifiers (and then we calculate the\n",
        "      mean or median of their estimations).\n",
        "\n",
        "You can run this code and print \"rf_top_dict\" and \"rf_ensemble_top_dict\" for better clarification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "dt_list = []\n",
        "for d in datasets:\n",
        "    dt_list.append(d.split('.csv')[0])\n",
        "\n",
        "rf_top_dict = {key: None for key in dt_list}\n",
        "\n",
        "rf_ensemble_top_dict = {}\n",
        "for key in dt_list:\n",
        "    rf_ensemble_top_dict[key] = []\n",
        "\n",
        "\n",
        "\n",
        "# This little variable controls the K in the top-K\n",
        "top = 5\n",
        "\n",
        "\n",
        "\n",
        "index = 0\n",
        "for dataset in rf_top_dict.keys():\n",
        "    quantifier_set = set()\n",
        "    ensemble_quantifier_list = []\n",
        "    ensemble_error_list = []\n",
        "    for i in range(0, top):\n",
        "        quantifier_set.add(algList[rf_table.loc[index]['rank-'+str(i+1)]])\n",
        "        ensemble_quantifier_list.append(algList[rf_table.loc[index]['rank-'+str(i+1)]])\n",
        "        ensemble_error_list.append(rf_table.loc[index]['abs-error-'+str( algList[rf_table.loc[index]['rank-'+str(i+1)]] )+'-predicted'])\n",
        "    rf_top_dict[dataset] = quantifier_set\n",
        "    \n",
        "    ensemble_numerator_list = []\n",
        "    denominator = 0\n",
        "    for error in ensemble_error_list:\n",
        "        denominator += 1/error\n",
        "        ensemble_numerator_list.append( 1/error )\n",
        "    \n",
        "    ensemble_weight_list = []\n",
        "    for numerator in ensemble_numerator_list:\n",
        "        ensemble_weight_list.append( numerator / denominator )\n",
        "\n",
        "    rf_ensemble_top_dict[dataset].append(ensemble_quantifier_list)\n",
        "    rf_ensemble_top_dict[dataset].append(ensemble_weight_list)\n",
        "    \n",
        "    index += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Top-K Ensemble HitRate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate the HitRate for the Top-K ensemble and save to the Recommender Hit Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf_top_hit_list = []\n",
        "index = 0\n",
        "for dataset in dt_list:\n",
        "    # if A_recommendation_set INTERSECTION A_optimal_set is not NULL, then a Hit is attributed\n",
        "    if len(rf_top_dict[dataset].intersection(optimal_dict[dataset])) != 0:\n",
        "        rf_top_hit_list.append(1)\n",
        "    else:\n",
        "        rf_top_hit_list.append(0)\n",
        "    index += 1\n",
        "\n",
        "rf_top_hit_rate = np.mean(rf_top_hit_list)\n",
        "\n",
        "\n",
        "# Save the HitRate to the Recommender Hit Table\n",
        "temp_table = pd.read_csv('./recommender_hit_table/recommender_hit_rate_table.csv')\n",
        "temp_table.loc[len(temp_table)] = ['Random Forest+Top'+str(top), rf_top_hit_rate]\n",
        "# temp_table.to_csv('./recommender_hit_table/recommender_hit_rate_table.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Top-K Ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code constructs the top-K ensemble method\n",
        "\n",
        "Each recommender predicts a MAE for their specific quantifier\n",
        "\n",
        "Quantifiers are then ordered by their MAE (crescent order)\n",
        "\n",
        "The Top-k are selected and are used to give their own estimates of the class prevalence {p_1, ..., p_k}\n",
        "\n",
        "The \"final\" prevalence is the mean OR median (we use both approaches) of the k estimations\n",
        "\n",
        "Then, the absolute error of our approach (mean or median) is calculated -> ABSOLUTE(real_prevalence - estimated_prevalence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_k_experiment_table_mean = pd.DataFrame(columns=['MAE', 'Dataset'])\n",
        "top_k_experiment_table_median = pd.DataFrame(columns=['MAE', 'Dataset'])\n",
        "\n",
        "dt_list = []\n",
        "for d in datasets:\n",
        "    dt_list.append(d.split('.csv')[0])\n",
        "\n",
        "index = 0\n",
        "for item in rf_top_dict.items():\n",
        "    quantifiers_set = item[1]\n",
        "    \n",
        "    pred_p_mean = [0] * 20\n",
        "    temp_pred_p_median = [ [] for _ in range(20) ]\n",
        "    pred_p_median = []\n",
        "    real_p = None\n",
        "    for x in quantifier_set:\n",
        "        # Get the real_p (prevalence) of the dataset and the predicted_p (prevalence) by the quantifier\n",
        "        real_p = experiment_tables_dict[x].loc[experiment_tables_dict[x]['dataset_name'] == dt_list[index]].groupby('alpha').mean(numeric_only=True)['real_p'].values.tolist()\n",
        "        pred_p = experiment_tables_dict[x].loc[experiment_tables_dict[x]['dataset_name'] == dt_list[index]].groupby('alpha').mean(numeric_only=True)['pred_p'].values.tolist()\n",
        "        for i in range(0, len(real_p)):\n",
        "            pred_p_mean[i] += pred_p[i]\n",
        "            temp_pred_p_median[i].append(pred_p[i])\n",
        "    \n",
        "    # The \"final\" prevalence will be the mean or median of each estimated prevalence by each quantifier\n",
        "    for i in range(0, len(pred_p_mean)):\n",
        "        pred_p_mean[i] /= len(quantifier_set)\n",
        "        pred_p_median.append(np.median(temp_pred_p_median[i]))\n",
        "    \n",
        "    abs_error_mean_list = []\n",
        "    abs_error_median_list = []\n",
        "    for i in range(0, len(pred_p_mean)):\n",
        "        abs_error_mean_list.append(abs(real_p[i] - pred_p_mean[i]))\n",
        "        abs_error_median_list.append(abs(real_p[i] - pred_p_median[i]))\n",
        "    abs_error_mean = np.mean(abs_error_mean_list)\n",
        "    abs_error_median = np.mean(abs_error_median_list)\n",
        "\n",
        "    top_k_experiment_table_mean.loc[len(top_k_experiment_table_mean)] = [abs_error_mean, dt_list[index]]\n",
        "    top_k_experiment_table_median.loc[len(top_k_experiment_table_median)] = [abs_error_median, dt_list[index]]\n",
        "    # experiment_table.loc[len(experiment_table)] = ['Random Forests+Top'+str(top)+' (MEAN)', dt_list[index], abs_error_mean]\n",
        "    # experiment_table.loc[len(experiment_table)] = ['Random Forests+Top'+str(top)+' (MEDIAN)', dt_list[index], abs_error_median]\n",
        "\n",
        "    index += 1\n",
        "\n",
        "if top == 1:\n",
        "    top_k_experiment_table_mean.to_csv('./experiment_tables/summarized/experiment_table_TOP'+str(top)+'.csv', index=False)\n",
        "else:\n",
        "    top_k_experiment_table_mean.to_csv('./experiment_tables/summarized/experiment_table_TOP'+str(top)+'_MEAN.csv', index=False)\n",
        "    top_k_experiment_table_median.to_csv('./experiment_tables/summarized/experiment_table_TOP'+str(top)+'_MEDIAN.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Top-K Weighted Ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code implements the weighted top-K ensemble method\n",
        "\n",
        "Each recommender predicts a MAE for their specific quantifier\n",
        "\n",
        "Quantifiers are then ordered by their MAE (crescent order)\n",
        "\n",
        "A weight is attributed to each quantifier based on their MAE. The lower the MAE, the bigger the weight. The first quantifier (Top-1) has the biggest weight, the second (Top-2) has the seccond biggest weight, and so on.\n",
        "\n",
        "ALL of the weights sum up to one -> weight_quantifier_1 + weight_quantifier_2 + ... + weight_quantifier_k = 1\n",
        "\n",
        "The Top-k are selected and are used to give it's own estimate of the class prevalence {p_1, ..., p_k}\n",
        "\n",
        "The \"final\" prevalence is the sum of the k estimations multiplied by their respective weight:\n",
        "\n",
        "prevalence_estimation = weight_quantifier_1 * prevalence_quantifier_1 + ... + weight_quantifier_k * prevalence_quantifier_k\n",
        "\n",
        "Then, the absolute error of our approach is calculated -> ABSOLUTE(real_prevalence - estimated_prevalence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_k_weighted_experiment_table = pd.DataFrame(columns=['MAE', 'Dataset'])\n",
        "\n",
        "dt_list = []\n",
        "for d in datasets:\n",
        "    dt_list.append(d.split('.csv')[0])\n",
        "\n",
        "index = 0\n",
        "for item in rf_ensemble_top_dict.items():\n",
        "    quantifier_list = item[1][0]\n",
        "    weight_list = item[1][1]\n",
        "    prevalence_list = [0] * 20\n",
        "\n",
        "    real_p = None\n",
        "    for x in quantifier_list:\n",
        "        # Get the real_p (prevalence) of the dataset and the predicted_p (prevalence) by the quantifier\n",
        "        real_p = experiment_tables_dict[x].loc[experiment_tables_dict[x]['dataset_name'] == dt_list[index]].groupby('alpha').mean(numeric_only=True)['real_p'].values.tolist()\n",
        "        pred_p = experiment_tables_dict[x].loc[experiment_tables_dict[x]['dataset_name'] == dt_list[index]].groupby('alpha').mean(numeric_only=True)['pred_p'].values.tolist()\n",
        "        \n",
        "        # The \"final\" prevalence will be the prevalences of the K quantifiers adjusted by their weights.\n",
        "        # Note that pred_p has 20 prevalences, since we test a quantifier on a dataset 20 times (varying the class distribution).\n",
        "        for i in range(0, len(pred_p)):\n",
        "            prevalence_list[i] += pred_p[i] * weight_list[quantifier_list.index(x)]\n",
        "    \n",
        "    ensemble_abs_error_list = []\n",
        "    for i in range(0, len(prevalence_list)):\n",
        "        ensemble_abs_error_list.append( abs( real_p[i] - prevalence_list[i] ) )\n",
        "\n",
        "    ensemble_abs_error = np.mean(ensemble_abs_error_list)\n",
        "    \n",
        "    top_k_weighted_experiment_table.loc[len(top_k_weighted_experiment_table)] = [ensemble_abs_error, dt_list[index]]\n",
        "    # experiment_table.loc[len(experiment_table)] = ['Random Forests+Top'+str(top)+'+Weighted (ENSEMBLE)', dt_list[index], ensemble_abs_error]\n",
        "    index += 1\n",
        "\n",
        "if top != 1: \n",
        "    top_k_weighted_experiment_table.to_csv('./experiment_tables/summarized/experiment_table_TOP'+str(top)+'_WEIGHTED.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unify all the Experiment Tables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the previous steps, we created Experiment Tables for each quantifier. The experiment tables for our Top-K approach were created here, while the experiment tables for the other quantifiers were created in the APP function and were summarized here (refer to 'summarized' folder).\n",
        "\n",
        "Now we are going to unify all these experiment tables into a single Experiment Table (even more summarized) consisting of a DataFrame containing (1) the method (algorithm), (2) the Dataset name and (3) MAE.\n",
        "\n",
        "This is just to facilitate our analysis and to condense all the results to a single file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The 'even more summarized' Experiment Table containts only the Method, Dataset and MAE\n",
        "even_more_summarized_experiment_table = pd.DataFrame(columns=['Method', 'Dataset', 'MAE'])\n",
        "\n",
        "dt_list = []\n",
        "for d in datasets:\n",
        "    dt_list.append(d.split('.csv')[0])\n",
        "\n",
        "for filename in os.listdir('./experiment_tables/summarized/'):\n",
        "    # We load each experiment table\n",
        "    df = pd.read_csv('./experiment_tables/summarized/'+filename)\n",
        "    string = filename.split('_')\n",
        "    method_name = string[2].split('.')[0]\n",
        "    \n",
        "    if 'TOP' in method_name and method_name != 'TOPLINE':\n",
        "        if '1' not in method_name:\n",
        "            method_name = method_name + '+' + string[3].split('.')[0]\n",
        "\n",
        "    # And then we include each row in the 'even more summarized' one\n",
        "    index = 0\n",
        "    for row in df.iterrows():\n",
        "        even_more_summarized_experiment_table.loc[len(even_more_summarized_experiment_table)] = [method_name, dt_list[index], row[1]['MAE']]\n",
        "        index += 1\n",
        "\n",
        "# Save the resulting 'even more summarized' experiment table\n",
        "even_more_summarized_experiment_table.to_csv('./experiment_tables/even_more_summarized/experiment_table.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "experiment_table = pd.read_csv('./experiment_tables/even_more_summarized/experiment_table_2.csv')\n",
        "old_experiment_table = pd.read_csv('./experiment_tables/even_more_summarized/experiment_table_3.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# old_experiment_table['Method'].replace({'Random Forests+Top1' : 'TOP1',\n",
        "#                                         'Random Forests+Top3 (MEAN)' : 'TOP3+MEAN',\n",
        "#                                         'Random Forests+Top3 (MEDIAN)' : 'TOP3+MEDIAN',\n",
        "#                                         'Random Forests+Top3+Weighted (ENSEMBLE)' : 'TOP3+WEIGHTED',\n",
        "#                                         'Random Forests+Top5 (MEAN)' : 'TOP5+MEAN',\n",
        "#                                         'Random Forests+TOP5 (MEDIAN)' : 'TOP5+MEDIAN',\n",
        "#                                         'Random Forests+Top5+Weighted (ENSEMBLE)' : 'TOP5+WEIGHTED',\n",
        "#                                         'Topline' : 'TOPLINE'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ACC : True\n",
            "CC : True\n",
            "DyS : True\n",
            "HDy : True\n",
            "MAX : True\n",
            "MS : True\n",
            "MS2 : True\n",
            "PACC : True\n",
            "PCC : True\n",
            "SMM : True\n",
            "SORD : True\n",
            "TOP1 : False\n",
            "TOP3+MEAN : False\n",
            "TOP3+MEDIAN : False\n",
            "TOP3+WEIGHTED : False\n",
            "TOP5+MEAN : False\n",
            "TOP5+MEDIAN : False\n",
            "TOP5+WEIGHTED : False\n",
            "TOPLINE : True\n",
            "X : True\n"
          ]
        }
      ],
      "source": [
        "for quantifier in experiment_table['Method'].unique().tolist():\n",
        "    print(quantifier + ' : ' + str(set(experiment_table.query(\"Method in ['\"+quantifier+\"']\")['MAE'].tolist()) == set(old_experiment_table.query(\"Method in ['\"+quantifier+\"']\")['MAE'].tolist())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAE_1 = experiment_table.query(\"Method in ['TOP1']\")['MAE'].tolist()\n",
        "MAE_2 = old_experiment_table.query(\"Method in ['TOP1']\")['MAE'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.04085,\n",
              " 0.1025,\n",
              " 0.0471,\n",
              " 0.5,\n",
              " 0.0271999999999999,\n",
              " 0.1624499999999999,\n",
              " 0.43655,\n",
              " 0.337,\n",
              " 0.0227499999999999,\n",
              " 0.263,\n",
              " 0.4755999999999999,\n",
              " 0.32255,\n",
              " 0.1075499999999999,\n",
              " 0.4544,\n",
              " 0.26505,\n",
              " 0.0,\n",
              " 0.1331999999999999,\n",
              " 0.0191999999999999,\n",
              " 0.0066,\n",
              " 0.27835,\n",
              " 0.0947,\n",
              " 0.0031499999999999,\n",
              " 0.0510499999999999,\n",
              " 0.08465,\n",
              " 0.0,\n",
              " 0.317,\n",
              " 0.5,\n",
              " 0.0319,\n",
              " 0.0202499999999999,\n",
              " 0.10305,\n",
              " 0.0773499999999999,\n",
              " 0.1075,\n",
              " 0.0561,\n",
              " 0.0200999999999999,\n",
              " 0.5,\n",
              " 0.0376,\n",
              " 0.0224999999999999,\n",
              " 0.0274,\n",
              " 0.01705,\n",
              " 0.04,\n",
              " 0.0104,\n",
              " 0.0069499999999999,\n",
              " 0.0009,\n",
              " 0.281,\n",
              " 0.0024499999999999,\n",
              " 0.01765,\n",
              " 0.0045999999999999,\n",
              " 0.0251,\n",
              " 0.482,\n",
              " 0.0252499999999999,\n",
              " 0.27265,\n",
              " 0.00905,\n",
              " 0.0232499999999999,\n",
              " 0.02235,\n",
              " 0.0905499999999999,\n",
              " 0.07235,\n",
              " 0.0286,\n",
              " 0.0774999999999999,\n",
              " 0.5,\n",
              " 0.5,\n",
              " 0.0315,\n",
              " 0.0894999999999999,\n",
              " 0.4005500000000001,\n",
              " 0.1421999999999999,\n",
              " 0.0843,\n",
              " 0.1036,\n",
              " 0.0452999999999999,\n",
              " 0.0638499999999999,\n",
              " 0.0238999999999999,\n",
              " 0.0030999999999999,\n",
              " 0.0264,\n",
              " 0.0064,\n",
              " 0.0135999999999999,\n",
              " 0.0479,\n",
              " 0.0488999999999999,\n",
              " 0.02095,\n",
              " 0.0144999999999999,\n",
              " 0.0066,\n",
              " 0.0180999999999999,\n",
              " 0.0406999999999999,\n",
              " 0.0043499999999999,\n",
              " 0.337,\n",
              " 0.02185,\n",
              " 0.0498,\n",
              " 0.5,\n",
              " 0.00815,\n",
              " 0.1752,\n",
              " 0.0034999999999999,\n",
              " 0.0057499999999999,\n",
              " 0.0026,\n",
              " 0.0486499999999999,\n",
              " 0.02365,\n",
              " 0.0369,\n",
              " 0.1681499999999999,\n",
              " 0.0624,\n",
              " 0.0046999999999999,\n",
              " 0.03865,\n",
              " 0.18955,\n",
              " 0.0088,\n",
              " 0.224,\n",
              " 0.04135,\n",
              " 0.0059,\n",
              " 0.0499999999999999,\n",
              " 0.4820000000000001,\n",
              " 0.0167499999999999,\n",
              " 0.3072,\n",
              " 0.1538,\n",
              " 0.01515,\n",
              " 0.0543,\n",
              " 0.0099,\n",
              " 0.0,\n",
              " 0.5,\n",
              " 0.0081999999999999,\n",
              " 0.01065,\n",
              " 0.0048999999999999,\n",
              " 0.0058,\n",
              " 0.00275,\n",
              " 0.005,\n",
              " 0.0617,\n",
              " 0.0534999999999999,\n",
              " 0.0205,\n",
              " 0.0459499999999999,\n",
              " 0.0672,\n",
              " 0.0103499999999999,\n",
              " 0.05155,\n",
              " 0.49115,\n",
              " 0.0235999999999999,\n",
              " 0.0134999999999999,\n",
              " 0.0062999999999999,\n",
              " 0.0023,\n",
              " 0.0233,\n",
              " 0.0080999999999999,\n",
              " 0.0024]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MAE_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.04085\t\t|\t\t0.0397 : 0.0011499999999999982\n",
            "0.1025\t\t|\t\t0.1409999999999999 : 0.03849999999999991\n",
            "0.0471\t\t|\t\t0.0193499999999999 : 0.027750000000000104\n",
            "0.5\t\t|\t\t0.5 : 0.0\n",
            "0.0271999999999999\t\t|\t\t0.0383999999999999 : 0.011199999999999998\n",
            "0.1624499999999999\t\t|\t\t0.1554499999999999 : 0.007000000000000006\n",
            "0.43655\t\t|\t\t0.1617499999999999 : 0.2748000000000001\n",
            "0.337\t\t|\t\t0.5 : 0.16299999999999998\n",
            "0.0227499999999999\t\t|\t\t0.0472 : 0.0244500000000001\n",
            "0.263\t\t|\t\t0.5 : 0.237\n",
            "0.4755999999999999\t\t|\t\t0.49895 : 0.023350000000000093\n",
            "0.32255\t\t|\t\t0.23645 : 0.08610000000000001\n",
            "0.1075499999999999\t\t|\t\t0.1088 : 0.0012500000000000983\n",
            "0.4544\t\t|\t\t0.5 : 0.045599999999999974\n",
            "0.26505\t\t|\t\t0.28385 : 0.018799999999999983\n",
            "0.0\t\t|\t\t0.0 : 0.0\n",
            "0.1331999999999999\t\t|\t\t0.08255 : 0.0506499999999999\n",
            "0.0191999999999999\t\t|\t\t0.0265499999999999 : 0.007349999999999999\n",
            "0.0066\t\t|\t\t0.00335 : 0.00325\n",
            "0.27835\t\t|\t\t0.1795499999999999 : 0.09880000000000008\n",
            "0.0947\t\t|\t\t0.3006 : 0.20589999999999997\n",
            "0.0031499999999999\t\t|\t\t0.0033499999999999 : 0.0002000000000000001\n",
            "0.0510499999999999\t\t|\t\t0.1147999999999999 : 0.06375\n",
            "0.08465\t\t|\t\t0.2968999999999999 : 0.21224999999999988\n",
            "0.0\t\t|\t\t0.0 : 0.0\n",
            "0.317\t\t|\t\t0.5 : 0.183\n",
            "0.5\t\t|\t\t0.5 : 0.0\n",
            "0.0319\t\t|\t\t0.03455 : 0.0026499999999999996\n",
            "0.0202499999999999\t\t|\t\t0.0225999999999999 : 0.0023500000000000014\n",
            "0.10305\t\t|\t\t0.09295 : 0.010099999999999998\n",
            "0.0773499999999999\t\t|\t\t0.06025 : 0.017099999999999907\n",
            "0.1075\t\t|\t\t0.1055499999999999 : 0.0019500000000001044\n",
            "0.0561\t\t|\t\t0.0347999999999999 : 0.021300000000000097\n",
            "0.0200999999999999\t\t|\t\t0.04645 : 0.0263500000000001\n",
            "0.5\t\t|\t\t0.5 : 0.0\n",
            "0.0376\t\t|\t\t0.05325 : 0.015649999999999997\n",
            "0.0224999999999999\t\t|\t\t0.0333 : 0.010800000000000105\n",
            "0.0274\t\t|\t\t0.0195999999999999 : 0.007800000000000102\n",
            "0.01705\t\t|\t\t0.0196 : 0.00255\n",
            "0.04\t\t|\t\t0.04375 : 0.0037499999999999964\n",
            "0.0104\t\t|\t\t0.01005 : 0.0003499999999999996\n",
            "0.0069499999999999\t\t|\t\t0.0444499999999999 : 0.0375\n",
            "0.0009\t\t|\t\t0.0 : 0.0009\n",
            "0.281\t\t|\t\t0.5 : 0.21899999999999997\n",
            "0.0024499999999999\t\t|\t\t0.0028499999999999 : 0.00039999999999999975\n",
            "0.01765\t\t|\t\t0.013 : 0.00465\n",
            "0.0045999999999999\t\t|\t\t0.01335 : 0.008750000000000101\n",
            "0.0251\t\t|\t\t0.02305 : 0.0020499999999999997\n",
            "0.482\t\t|\t\t0.5 : 0.018000000000000016\n",
            "0.0252499999999999\t\t|\t\t0.0218 : 0.003449999999999901\n",
            "0.27265\t\t|\t\t0.26595 : 0.006699999999999984\n",
            "0.00905\t\t|\t\t0.0154499999999999 : 0.006399999999999899\n",
            "0.0232499999999999\t\t|\t\t0.0576999999999999 : 0.034449999999999995\n",
            "0.02235\t\t|\t\t0.04385 : 0.021500000000000002\n",
            "0.0905499999999999\t\t|\t\t0.0322 : 0.058349999999999895\n",
            "0.07235\t\t|\t\t0.01635 : 0.055999999999999994\n",
            "0.0286\t\t|\t\t0.0375999999999999 : 0.008999999999999897\n",
            "0.0774999999999999\t\t|\t\t0.07745 : 4.999999999989735e-05\n",
            "0.5\t\t|\t\t0.5 : 0.0\n",
            "0.5\t\t|\t\t0.5 : 0.0\n",
            "0.0315\t\t|\t\t0.2473 : 0.2158\n",
            "0.0894999999999999\t\t|\t\t0.24225 : 0.1527500000000001\n",
            "0.4005500000000001\t\t|\t\t0.5 : 0.09944999999999993\n",
            "0.1421999999999999\t\t|\t\t0.0672 : 0.07499999999999991\n",
            "0.0843\t\t|\t\t0.06675 : 0.017549999999999996\n",
            "0.1036\t\t|\t\t0.0747 : 0.028899999999999995\n",
            "0.0452999999999999\t\t|\t\t0.0282 : 0.017099999999999903\n",
            "0.0638499999999999\t\t|\t\t0.0571499999999999 : 0.0067000000000000046\n",
            "0.0238999999999999\t\t|\t\t0.01455 : 0.0093499999999999\n",
            "0.0030999999999999\t\t|\t\t0.0057999999999999 : 0.0026999999999999997\n",
            "0.0264\t\t|\t\t0.0066499999999999 : 0.0197500000000001\n",
            "0.0064\t\t|\t\t0.004 : 0.0024000000000000002\n",
            "0.0135999999999999\t\t|\t\t0.01415 : 0.000550000000000099\n",
            "0.0479\t\t|\t\t0.0364 : 0.011499999999999996\n",
            "0.0488999999999999\t\t|\t\t0.02805 : 0.020849999999999903\n",
            "0.02095\t\t|\t\t0.0095499999999999 : 0.0114000000000001\n",
            "0.0144999999999999\t\t|\t\t0.0270499999999999 : 0.01255\n",
            "0.0066\t\t|\t\t0.0071999999999999 : 0.0005999999999999001\n",
            "0.0180999999999999\t\t|\t\t0.05985 : 0.0417500000000001\n",
            "0.0406999999999999\t\t|\t\t0.18805 : 0.1473500000000001\n",
            "0.0043499999999999\t\t|\t\t0.0083499999999999 : 0.003999999999999999\n",
            "0.337\t\t|\t\t0.5 : 0.16299999999999998\n",
            "0.02185\t\t|\t\t0.1153 : 0.09345\n",
            "0.0498\t\t|\t\t0.0696 : 0.019799999999999998\n",
            "0.5\t\t|\t\t0.5 : 0.0\n",
            "0.00815\t\t|\t\t0.1972999999999999 : 0.1891499999999999\n",
            "0.1752\t\t|\t\t0.1949 : 0.019699999999999995\n",
            "0.0034999999999999\t\t|\t\t0.0023999999999999 : 0.0010999999999999998\n",
            "0.0057499999999999\t\t|\t\t0.0099499999999999 : 0.0042\n",
            "0.0026\t\t|\t\t0.0037499999999999 : 0.0011499999999999002\n",
            "0.0486499999999999\t\t|\t\t0.0516 : 0.0029500000000000984\n",
            "0.02365\t\t|\t\t0.02455 : 0.000899999999999998\n",
            "0.0369\t\t|\t\t0.03385 : 0.003050000000000004\n",
            "0.1681499999999999\t\t|\t\t0.10055 : 0.06759999999999991\n",
            "0.0624\t\t|\t\t0.2496999999999999 : 0.1872999999999999\n",
            "0.0046999999999999\t\t|\t\t0.0219 : 0.0172000000000001\n",
            "0.03865\t\t|\t\t0.03985 : 0.0011999999999999997\n",
            "0.18955\t\t|\t\t0.11975 : 0.0698\n",
            "0.0088\t\t|\t\t0.0091 : 0.0002999999999999999\n",
            "0.224\t\t|\t\t0.2398 : 0.01580000000000001\n",
            "0.04135\t\t|\t\t0.04115 : 0.0001999999999999988\n",
            "0.0059\t\t|\t\t0.00325 : 0.00265\n",
            "0.0499999999999999\t\t|\t\t0.0388999999999999 : 0.011099999999999999\n",
            "0.4820000000000001\t\t|\t\t0.5 : 0.017999999999999905\n",
            "0.0167499999999999\t\t|\t\t0.0259999999999999 : 0.009250000000000001\n",
            "0.3072\t\t|\t\t0.3069 : 0.00029999999999996696\n",
            "0.1538\t\t|\t\t0.1258999999999999 : 0.02790000000000009\n",
            "0.01515\t\t|\t\t0.0280999999999999 : 0.012949999999999899\n",
            "0.0543\t\t|\t\t0.0372 : 0.017100000000000004\n",
            "0.0099\t\t|\t\t0.0123499999999999 : 0.0024499999999998984\n",
            "0.0\t\t|\t\t0.0 : 0.0\n",
            "0.5\t\t|\t\t0.5 : 0.0\n",
            "0.0081999999999999\t\t|\t\t0.0095499999999999 : 0.0013500000000000005\n",
            "0.01065\t\t|\t\t0.0153 : 0.00465\n",
            "0.0048999999999999\t\t|\t\t0.005 : 0.00010000000000010001\n",
            "0.0058\t\t|\t\t0.00655 : 0.0007500000000000007\n",
            "0.00275\t\t|\t\t0.00345 : 0.0007000000000000001\n",
            "0.005\t\t|\t\t0.00465 : 0.0003500000000000005\n",
            "0.0617\t\t|\t\t0.4985 : 0.4368\n",
            "0.0534999999999999\t\t|\t\t0.0264999999999999 : 0.027000000000000003\n",
            "0.0205\t\t|\t\t0.02 : 0.0005000000000000004\n",
            "0.0459499999999999\t\t|\t\t0.06275 : 0.0168000000000001\n",
            "0.0672\t\t|\t\t0.0881 : 0.020900000000000002\n",
            "0.0103499999999999\t\t|\t\t0.00965 : 0.0006999999999998986\n",
            "0.05155\t\t|\t\t0.0722 : 0.02065\n",
            "0.49115\t\t|\t\t0.5 : 0.008850000000000025\n",
            "0.0235999999999999\t\t|\t\t0.0166499999999999 : 0.006949999999999998\n",
            "0.0134999999999999\t\t|\t\t0.0127 : 0.0007999999999998997\n",
            "0.0062999999999999\t\t|\t\t0.0065499999999999 : 0.00024999999999999935\n",
            "0.0023\t\t|\t\t0.00265 : 0.00035000000000000005\n",
            "0.0233\t\t|\t\t0.04895 : 0.02565\n",
            "0.0080999999999999\t\t|\t\t0.00835 : 0.0002500000000000991\n",
            "0.0024\t\t|\t\t0.0032 : 0.0008000000000000004\n"
          ]
        }
      ],
      "source": [
        "i = 0\n",
        "for i in range(0, len(MAE_1)):\n",
        "    print(str(MAE_1[i]) + '\\t\\t|\\t\\t' + str(MAE_2[i]) + ' : ' + str(abs(MAE_1[i] - MAE_2[i])))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ACC',\n",
              " 'CC',\n",
              " 'DyS',\n",
              " 'HDy',\n",
              " 'MAX',\n",
              " 'MS',\n",
              " 'MS2',\n",
              " 'PACC',\n",
              " 'PCC',\n",
              " 'SMM',\n",
              " 'SORD',\n",
              " 'TOP1',\n",
              " 'TOP3+MEAN',\n",
              " 'TOP3+MEDIAN',\n",
              " 'TOP3+WEIGHTED',\n",
              " 'TOP5+MEAN',\n",
              " 'TOP5+MEDIAN',\n",
              " 'TOP5+WEIGHTED',\n",
              " 'TOPLINE',\n",
              " 'X']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "experiment_table['Method'].unique().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "old_experiment_table['Method'].replace({'Random Forests+Top1' : 'TOP1',\n",
        "                                        'Random Forests+Top3 (MEAN)' : 'TOP3+MEAN',\n",
        "                                        'Random Forests+Top3 (MEDIAN)' : 'TOP3+MEDIAN',\n",
        "                                        'Random Forests+Top3+Weighted (ENSEMBLE)' : 'TOP3+WEIGHTED',\n",
        "                                        'Random Forests+Top5 (MEAN)' : 'TOP5+MEAN',\n",
        "                                        'Random Forests+TOP5 (MEDIAN)' : 'TOP5+MEDIAN',\n",
        "                                        'Random Forests+Top5+Weighted (ENSEMBLE)' : 'TOP5+WEIGHTED'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['ACC', 'CC', 'DyS', 'HDy', 'MAX', 'MS', 'MS2', 'PACC', 'PCC',\n",
              "       'SMM', 'SORD', 'TOP1', 'TOP3+MEAN', 'TOP3+MEDIAN', 'TOP3+WEIGHTED',\n",
              "       'TOP5+MEAN', 'TOP5+MEDIAN', 'TOP5+WEIGHTED', 'TOPLINE', 'X'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "old_experiment_table['Method'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.1832,\n",
              " 0.19905,\n",
              " 0.28845,\n",
              " 0.5,\n",
              " 0.27,\n",
              " 0.38815,\n",
              " 0.33715,\n",
              " 0.5,\n",
              " 0.12955,\n",
              " 0.5,\n",
              " 0.4993999999999999,\n",
              " 0.43395,\n",
              " 0.1975,\n",
              " 0.5,\n",
              " 0.3929999999999999,\n",
              " 0.0,\n",
              " 0.37205,\n",
              " 0.0317,\n",
              " 0.00885,\n",
              " 0.3398,\n",
              " 0.42095,\n",
              " 0.00955,\n",
              " 0.4485,\n",
              " 0.3601,\n",
              " 0.0,\n",
              " 0.5,\n",
              " 0.5,\n",
              " 0.1902499999999999,\n",
              " 0.22625,\n",
              " 0.0762,\n",
              " 0.215,\n",
              " 0.4203499999999999,\n",
              " 0.22795,\n",
              " 0.24815,\n",
              " 0.5,\n",
              " 0.08525,\n",
              " 0.0233999999999999,\n",
              " 0.14825,\n",
              " 0.13335,\n",
              " 0.22665,\n",
              " 0.00975,\n",
              " 0.02005,\n",
              " 0.0,\n",
              " 0.5,\n",
              " 0.0151,\n",
              " 0.1303,\n",
              " 0.01665,\n",
              " 0.0538999999999999,\n",
              " 0.5,\n",
              " 0.3148999999999999,\n",
              " 0.2978,\n",
              " 0.00995,\n",
              " 0.16845,\n",
              " 0.2118,\n",
              " 0.217,\n",
              " 0.2162,\n",
              " 0.20975,\n",
              " 0.34765,\n",
              " 0.5,\n",
              " 0.5,\n",
              " 0.4853499999999999,\n",
              " 0.47485,\n",
              " 0.5,\n",
              " 0.25215,\n",
              " 0.1448,\n",
              " 0.2146,\n",
              " 0.3625,\n",
              " 0.25295,\n",
              " 0.02845,\n",
              " 0.02135,\n",
              " 0.02535,\n",
              " 0.0086,\n",
              " 0.0204,\n",
              " 0.03705,\n",
              " 0.22735,\n",
              " 0.05325,\n",
              " 0.0786,\n",
              " 0.02075,\n",
              " 0.1417,\n",
              " 0.22195,\n",
              " 0.02845,\n",
              " 0.5,\n",
              " 0.2161,\n",
              " 0.1877499999999999,\n",
              " 0.5,\n",
              " 0.02555,\n",
              " 0.32205,\n",
              " 0.0103,\n",
              " 0.0592,\n",
              " 0.0,\n",
              " 0.173,\n",
              " 0.1582,\n",
              " 0.16365,\n",
              " 0.2196999999999999,\n",
              " 0.36655,\n",
              " 0.011,\n",
              " 0.0917999999999999,\n",
              " 0.38775,\n",
              " 0.0483,\n",
              " 0.33525,\n",
              " 0.3703,\n",
              " 0.0052,\n",
              " 0.02155,\n",
              " 0.5,\n",
              " 0.05615,\n",
              " 0.2656,\n",
              " 0.1422,\n",
              " 0.0291,\n",
              " 0.1339499999999999,\n",
              " 0.0768,\n",
              " 0.0,\n",
              " 0.5,\n",
              " 0.03675,\n",
              " 0.10295,\n",
              " 0.0151,\n",
              " 0.0098,\n",
              " 0.0131,\n",
              " 0.09005,\n",
              " 0.03975,\n",
              " 0.4985,\n",
              " 0.22745,\n",
              " 0.1285,\n",
              " 0.3728,\n",
              " 0.2963,\n",
              " 0.4313499999999999,\n",
              " 0.5,\n",
              " 0.1835,\n",
              " 0.0954,\n",
              " 0.0416,\n",
              " 0.0094,\n",
              " 0.24645,\n",
              " 0.0491,\n",
              " 0.004]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "old_experiment_table.query(\"Method in ['CC']\")['MAE'].tolist()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Mq8fYR6NY_7_",
        "txTPtjMCJTyg",
        "eHUI9P42ZC5o"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
