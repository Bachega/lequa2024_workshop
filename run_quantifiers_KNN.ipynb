{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq8fYR6NY_7_"
      },
      "source": [
        "# init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmUgYPsj0Tw-",
        "outputId": "a2bb0820-5744-477f-bfe7-24e61b7dce92"
      },
      "outputs": [],
      "source": [
        "from quantifiers.ACC import ACC\n",
        "from quantifiers.dys_method import dys_method\n",
        "from quantifiers.MS import MS_method\n",
        "\n",
        "from utils.getTrainingScores import getTrainingScores\n",
        "from utils.getTPRFPR import getTPRFPR\n",
        "from utils.applyquantifiers import apply_quantifier\n",
        "from utils.fitQuantifierSchumacherGithub import fitQuantifierSchumacherGithub\n",
        "\n",
        "import pdb\n",
        "import quapy as qp\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from scipy.io.arff import loadarff\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.calibration import CalibrationDisplay\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "meta_table_path = './metafeatures/meta-features-table.csv'\n",
        "path = \"./datasets/\"\n",
        "train_data_path = \"./train_data/\"\n",
        "test_data_path = \"./test_data/\"\n",
        "# files = os.listdir(path)\n",
        "meta_features_table_index = pd.read_csv('./metafeatures/meta-features-table-index.csv')\n",
        "files = meta_features_table_index.pop('dataset_name').tolist()\n",
        "\n",
        "counters = ['CC', 'ACC', 'PACC', 'PCC', 'SMM', 'HDy', 'DyS', 'SORD', 'MS', 'MS2', 'MAX', 'X', 'T50']\n",
        "# counters = [\"CC\",\"ACC\",\"SMM\",\"HDy\",\"DyS\",\"SORD\",\"MS\",\"MS2\",\"MAX\",\"X\",\"T50\",\"PCC\",\"PACC\",\"GAC\",\"GPAC\",\"FM\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHUI9P42ZC5o"
      },
      "source": [
        "# preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdhBBmoIN0ew"
      },
      "outputs": [],
      "source": [
        "# i = 0\n",
        "# dataframe = None\n",
        "# X = None\n",
        "# y = None\n",
        "# X_list = []\n",
        "# y_list = []\n",
        "\n",
        "# for f in files:\n",
        "#   df = pd.read_csv(path + f)\n",
        "#   df = df.dropna()\n",
        "  \n",
        "#   y = df.pop(df.columns[-1])\n",
        "#   X = df\n",
        "\n",
        "#   y_list.append(y.to_numpy())\n",
        "#   X_list.append(X.to_numpy())\n",
        "\n",
        "#   i += 1\n",
        "# i = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "i = 0\n",
        "dataframe = None\n",
        "X_train = None\n",
        "y_train = None\n",
        "X_test = None\n",
        "y_test = None\n",
        "X_train_list = []\n",
        "y_train_list = []\n",
        "X_test_list = []\n",
        "y_test_list = []\n",
        "\n",
        "for f in files:\n",
        "  \n",
        "  # TRAIN DATA\n",
        "  df_train = pd.read_csv(train_data_path + str(f.split('.csv')[0]) + '-TRAIN.csv')\n",
        "  df_train = df_train.dropna()\n",
        "  \n",
        "  y_train = df_train.pop(df_train.columns[-1])\n",
        "  X_train = df_train\n",
        "\n",
        "  y_train_list.append(y_train.to_numpy())\n",
        "  X_train_list.append(X_train.to_numpy())\n",
        "\n",
        "\n",
        "  # TEST DATA\n",
        "  df_test = pd.read_csv(test_data_path + str(f.split('.csv')[0]) + '-TEST.csv')\n",
        "  df_test = df_test.dropna()\n",
        "  \n",
        "  y_test = df_test.pop(df_test.columns[-1])\n",
        "  X_test = df_test\n",
        "\n",
        "  y_test_list.append(y_test.to_numpy())\n",
        "  X_test_list.append(X_test.to_numpy())\n",
        "\n",
        "  i += 1\n",
        "i = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVSD81CqPdjF"
      },
      "outputs": [],
      "source": [
        "meta_features_table = pd.read_csv(meta_table_path)\n",
        "\n",
        "if os.path.isfile('./metafeatures/meta-table.csv'):\n",
        "    meta_table = pd.read_csv('./metafeatures/meta-table.csv')\n",
        "else:\n",
        "    meta_table_columns = meta_features_table.columns.tolist()\n",
        "    for counter in counters:\n",
        "        meta_table_columns.append('arr_' + counter)\n",
        "    meta_table = pd.DataFrame(columns=meta_table_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # meta_table\n",
        "\n",
        "# meta_table.loc[len(meta_table.index)] = meta_features_table.iloc[0].tolist()\n",
        "# meta_table.loc[len(meta_table.index)] = meta_features_table.iloc[1].tolist()\n",
        "# meta_table.loc[len(meta_table.index)] = meta_features_table.iloc[2].tolist()\n",
        "# meta_table.loc[len(meta_table.index)] = meta_features_table.iloc[3].tolist()\n",
        "# meta_table.loc[len(meta_table.index)] = meta_features_table.iloc[4].tolist()\n",
        "\n",
        "# meta_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# meta_table_columns = meta_features_table.columns.tolist()\n",
        "# for counter in counters:\n",
        "#     meta_table_columns.append('arr_' + counter)\n",
        "# meta_table_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMQQS-h31g3c"
      },
      "source": [
        "# meta-features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5laWnKwkJh_X"
      },
      "outputs": [],
      "source": [
        "def run_experiment(X_train, y_train, X_test, y_test, dataset_name):\n",
        "  #......................input/output path directories....................\n",
        "\n",
        "  # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "  \n",
        "  clf = None\n",
        "  # clf = LogisticRegression(random_state=42, n_jobs=-1)\n",
        "  # clf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "  \n",
        "  calib_clf = CalibratedClassifierCV(clf, cv=3, n_jobs=-1)\n",
        "  calib_clf.fit(X_train, y_train)\n",
        "\n",
        "  try:\n",
        "    clf = joblib.load('./estimator_parameters/' + dataset_name + '.joblib')\n",
        "    clf.n_jobs = -1\n",
        "  except:\n",
        "    clf = LogisticRegression(random_state=42, n_jobs=-1)\n",
        "\n",
        "  scores = getTrainingScores(X_train, y_train, 10, clf)[0] # None\n",
        "  tprfpr = getTPRFPR(scores)\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "  # disp = CalibrationDisplay.from_estimator(rf_clf, X_test, y_test)\n",
        "  # disp.plot()\n",
        "\n",
        "  # disp = CalibrationDisplay.from_estimator(calib_clf, X_test, y_test)\n",
        "  # disp.plot()\n",
        "\n",
        "  # plt.show()\n",
        "\n",
        "  niterations = 10 # how many replicates it will take\n",
        "  batch_sizes = list([100]) # list(range(10, min(91, max_allowed + 1), 10))# + list(range(100, min(501, max_allowed + 1), 100)) # test set sizes\n",
        "  alpha_values = [round(x, 2) for x in np.linspace(0,1,20)]   # class proportion\n",
        "  # alpha_values = [x for x in alpha_values if x < 0.4 or x > 0.6]  # removing alpha > 0.4 and alpha < 0.6\n",
        "\n",
        "  pos_scores = scores[scores[\"class\"]==1][\"scores\"]\n",
        "  neg_scores = scores[scores[\"class\"]==0][\"scores\"]\n",
        "  \n",
        "  X_test = pd.DataFrame(X_test)\n",
        "  y_test = pd.DataFrame(y_test, columns=[str(len(X_test.columns))])\n",
        "  df_test = pd.concat([X_test, y_test], axis=1)\n",
        "  \n",
        "  # WAS ZERO (0) BEFORE\n",
        "  df_test_pos = df_test.loc[df_test[df_test.columns[-1]] == 1] # seperating positive test examples\n",
        "  df_test_neg = df_test.loc[df_test[df_test.columns[-1]] == 0] # seperating negative test examples\n",
        "  \n",
        "  # pdb.set_trace()\n",
        "\n",
        "  table=pd.DataFrame(columns=['quantifier', 'abs-error', 'execution-time'])\n",
        "  for sample_size in batch_sizes:   # [10,100,500], batch_sizes, Varying test set sizes\n",
        "\n",
        "    for alpha in alpha_values: # Varying positive class distribution\n",
        "      # abs_error_cc = []\n",
        "      # abs_error_dys = []\n",
        "\n",
        "      error = []\n",
        "\n",
        "      for iter in range(niterations):\n",
        "        pos_size = int(round(sample_size * alpha, 2))\n",
        "        neg_size = sample_size - pos_size\n",
        "\n",
        "        #\n",
        "        # AVISAR O PROF ANDRE QUE SÃ“ FUNCIONOU COM REPLACE = TRUE\n",
        "        #\n",
        "        # df_test_neg\n",
        "        # sample_test_pos = df_test_pos.sample( int(pos_size), replace = False)\n",
        "        # sample_test_neg = df_test_neg.sample( int(neg_size), replace = False)\n",
        "        sample_test_pos = df_test_pos.sample( int(pos_size), replace = True)\n",
        "        sample_test_neg = df_test_neg.sample( int(neg_size), replace = True)\n",
        "\n",
        "        sample_test = pd.concat([sample_test_pos, sample_test_neg])\n",
        "\n",
        "        test_label = sample_test[sample_test.columns[-1]] # sample_test[\"class\"]\n",
        "\n",
        "        test_sample = sample_test.drop([sample_test.columns[-1]], axis=1) # sample_test.drop([\"class\"], axis=1)  #dropping class label columns\n",
        "        te_scores = clf.predict_proba(test_sample)[:,1]  #estimating test sample scores\n",
        "\n",
        "        n_pos_sample_test = list(test_label).count(1) #Counting num of actual positives in test sample\n",
        "        calcultd_pos_prop = round(n_pos_sample_test/len(sample_test), 2) #actual pos class prevalence in generated sample\n",
        "\n",
        "        # print(counters)\n",
        "        for quantifier in counters:\n",
        "          #..............Test Sample QUAPY exp...........................\n",
        "          te_quapy = None\n",
        "          external_qnt = None\n",
        "          #if quantifier in ['EM', 'PWK']:\n",
        "          #  print('ok')\n",
        "          #  external_qnt = fitQuantifierSchumacherGithub(quantifier, X_train, y_train)                  \n",
        "          #  te_quapy = qp.data.LabelledCollection(sample_test.drop([\"class\",\"Binary_label\"], axis=1), test_label)\n",
        "          \n",
        "          #.............Calling of Methods..................................................\n",
        "          start = time.time()\n",
        "          pred_pos_prop = apply_quantifier(qntMethod=quantifier,\n",
        "                                           clf=calib_clf,\n",
        "                                           scores=scores,\n",
        "                                           p_score=pos_scores,\n",
        "                                           n_score=neg_scores,\n",
        "                                           train_labels=None,\n",
        "                                           test_score=te_scores,\n",
        "                                           TprFpr=tprfpr,\n",
        "                                           thr=0.5,\n",
        "                                           measure='hellinger',\n",
        "                                           test_data=test_sample,\n",
        "                                           test_quapy=te_quapy,\n",
        "                                           external_qnt=external_qnt) #y_test=test_label\n",
        "          stop = time.time()\n",
        "          t = stop - start\n",
        "\n",
        "          pred_pos_prop = np.round(pred_pos_prop,2)  #predicted class proportion\n",
        "          \n",
        "          #..............................RESULTS Evaluation.....................................\n",
        "          abs_error = round(abs(calcultd_pos_prop - pred_pos_prop), 2) # absolute error\n",
        "          \n",
        "          table.loc[len(table)] = [quantifier,abs_error,t]\n",
        "          # error = round(calcultd_pos_prop - pred_pos_prop, 2)     # simple error Biasness\n",
        "          \n",
        "  return table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2PwBTyQNRbn",
        "outputId": "08d1f61b-efcd-43dc-eede-44ff7e6d0c28"
      },
      "outputs": [],
      "source": [
        "skip_count = 0\n",
        "table = None\n",
        "\n",
        "result = {}\n",
        "for counter in counters:\n",
        "    result[counter] = []\n",
        "\n",
        "file = open('log.txt', 'w')\n",
        "file.close()\n",
        "\n",
        "result_index = 0\n",
        "for i in range(len(meta_table), len(X_train_list)):\n",
        "  try:\n",
        "    table = run_experiment(X_train_list[i], y_train_list[i], X_test_list[i], y_test_list[i], str(files[i].split('.csv')[0]))\n",
        "\n",
        "    table = table.groupby('quantifier')['abs-error', 'execution-time'].aggregate('mean')\n",
        "\n",
        "    alpha = 0\n",
        "    for key in counters:\n",
        "      try:\n",
        "        sum = 0\n",
        "        for k in counters:\n",
        "          if k != key:\n",
        "            sum += ((1 - table['abs-error'][key]) / (1 - table['abs-error'][k]))\n",
        "            # sum += ((1 - table['abs-error'][key]) / (1 - table['abs-error'][k])) / (1 + alpha * math.log((table['execution-time'][key] / table['execution-time'][k])))\n",
        "        arr = sum / (len(counters) - 1)\n",
        "        result[key].append(arr)\n",
        "      except:\n",
        "        result[key].append(-1)    \n",
        "\n",
        "    row = meta_features_table.iloc[i].tolist()\n",
        "    for key in result:\n",
        "      row.append(result[key][result_index])\n",
        "    meta_table.loc[len(meta_table.index)] = row\n",
        "    \n",
        "    meta_table.to_csv('./metafeatures/meta-table.csv', index = False)\n",
        "\n",
        "    result_index += 1\n",
        "    print('Finished ' + str(i))\n",
        "  except Exception as e:\n",
        "    print('Skipping ' + str(i) + '...\\t\\t\\t' + str(e))\n",
        "    # for key in meta_table_dict:\n",
        "    #   meta_table_dict[key].drop(i, inplace = True)\n",
        "    skip_count += 1\n",
        "    \n",
        "    file = open('log.txt', 'a')\n",
        "    file.write('Skipping ' + str(i) + '...\\t\\t\\t' + str(e) + '\\n')\n",
        "    file.write('Dataset: ' + str(files[i]) + '\\n')\n",
        "    file.write('table[abs-error][key]:\\n' + str(table['abs-error'][key]) + '\\n')\n",
        "    file.write('table[abs-error][k]:\\n' + str(table['abs-error'][k]) + '\\n')\n",
        "    file.write('table[execution-time][key]:\\n' + str(table['execution-time'][key]) + '\\n')\n",
        "    file.write('table[execution-time][k]:\\n' + str(table['execution-time'][k]) + '\\n')\n",
        "    file.write(str(table['execution-time'][key] / table['execution-time'][k]) + '\\n')\n",
        "    file.write('\\n')\n",
        "    file.close()\n",
        "\n",
        "\n",
        "  # # TEST\n",
        "  # if i == 6:\n",
        "  #   break\n",
        "\n",
        "# for key in meta_table_dict:\n",
        "#   meta_table_dict[key]['arr'] = result[key]\n",
        "\n",
        "# for key in result:\n",
        "#     meta_features_table[('arr_' + key)] = result[key]\n",
        "\n",
        "print('\\n\\nSkipped ' + str(skip_count) + ' dataset(s)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# meta_features_table.to_csv('./metafeatures/meta-table.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pickle\n",
        "\n",
        "# with open('result.pkl', 'wb') as fp:\n",
        "#     pickle.dump(result, fp)\n",
        "#     print('dictionary saved successfully to file')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with open('result.pkl', 'rb') as fp:\n",
        "#     person = pickle.load(fp)\n",
        "#     print(person)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# result\n",
        "# meta_table_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# finSavePath = './metafeatures/meta-table'\n",
        "# for c in counters:\n",
        "#     meta_table_dict[c].to_csv(str(finSavePath + '-' + c + '.csv'), index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# test = pd.read_csv('./metafeatures/meta-table-PACC.csv')\n",
        "# test = test.pop(test.columns[-1])\n",
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "\n",
        "# i = 0\n",
        "# for t in test:\n",
        "#     if np.isnan(t):\n",
        "#         print(i)\n",
        "#     i += 1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
