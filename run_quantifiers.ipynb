{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# gambiarra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq8fYR6NY_7_"
      },
      "source": [
        "# init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmUgYPsj0Tw-",
        "outputId": "a2bb0820-5744-477f-bfe7-24e61b7dce92"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './metafeatures/meta-features-table-index.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m test_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./test_data/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# files = os.listdir(path)\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m meta_features_table_index \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./metafeatures/meta-features-table-index.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m files \u001b[38;5;241m=\u001b[39m meta_features_table_index\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_name\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     39\u001b[0m counters \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mACC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPACC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPCC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSMM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHDy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDyS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSORD\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMS2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT50\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './metafeatures/meta-features-table-index.csv'"
          ]
        }
      ],
      "source": [
        "# from CC import classify_count\n",
        "# from quantifiers.ACC import ACC\n",
        "# from quantifiers.dys_method import dys_method\n",
        "# from quantifiers.MS import MS_method\n",
        "\n",
        "# from utils.getTrainingScores import getTrainingScores\n",
        "# from utils.getTPRFPR import getTPRFPR\n",
        "# from utils.applyquantifiers import apply_quantifier\n",
        "# from utils.fitQuantifierSchumacherGithub import fitQuantifierSchumacherGithub\n",
        "\n",
        "import pdb\n",
        "# import quapy as qp\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from scipy.io.arff import loadarff\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "# from sklearn.calibration import CalibrationDisplay\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "meta_table_path = './metafeatures/meta-features-table.csv'\n",
        "path = \"./datasets/\"\n",
        "train_data_path = \"./train_data/\"\n",
        "test_data_path = \"./test_data/\"\n",
        "# files = os.listdir(path)\n",
        "meta_features_table_index = pd.read_csv('./metafeatures/meta-features-table-index.csv')\n",
        "files = meta_features_table_index.pop('dataset_name').tolist()\n",
        "\n",
        "counters = ['CC', 'ACC', 'PACC', 'PCC', 'SMM', 'HDy', 'DyS', 'SORD', 'MS', 'MS2', 'MAX', 'X', 'T50']\n",
        "# counters = [\"CC\",\"ACC\",\"SMM\",\"HDy\",\"DyS\",\"SORD\",\"MS\",\"MS2\",\"MAX\",\"X\",\"T50\",\"PCC\",\"PACC\",\"GAC\",\"GPAC\",\"FM\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHUI9P42ZC5o"
      },
      "source": [
        "# preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdhBBmoIN0ew"
      },
      "outputs": [],
      "source": [
        "# i = 0\n",
        "# dataframe = None\n",
        "# X = None\n",
        "# y = None\n",
        "# X_list = []\n",
        "# y_list = []\n",
        "\n",
        "# for f in files:\n",
        "#   df = pd.read_csv(path + f)\n",
        "#   df = df.dropna()\n",
        "  \n",
        "#   y = df.pop(df.columns[-1])\n",
        "#   X = df\n",
        "\n",
        "#   y_list.append(y.to_numpy())\n",
        "#   X_list.append(X.to_numpy())\n",
        "\n",
        "#   i += 1\n",
        "# i = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "i = 0\n",
        "dataframe = None\n",
        "X_train = None\n",
        "y_train = None\n",
        "X_test = None\n",
        "y_test = None\n",
        "X_train_list = []\n",
        "y_train_list = []\n",
        "X_test_list = []\n",
        "y_test_list = []\n",
        "\n",
        "for f in files:\n",
        "  \n",
        "  # TRAIN DATA\n",
        "  df_train = pd.read_csv(train_data_path + str(f.split('.csv')[0]) + '-TRAIN.csv')\n",
        "  df_train = df_train.dropna()\n",
        "  \n",
        "  y_train = df_train.pop(df_train.columns[-1])\n",
        "  X_train = df_train\n",
        "\n",
        "  y_train_list.append(y_train.to_numpy())\n",
        "  X_train_list.append(X_train.to_numpy())\n",
        "\n",
        "\n",
        "  # TEST DATA\n",
        "  df_test = pd.read_csv(test_data_path + str(f.split('.csv')[0]) + '-TEST.csv')\n",
        "  df_test = df_test.dropna()\n",
        "  \n",
        "  y_test = df_test.pop(df_test.columns[-1])\n",
        "  X_test = df_test\n",
        "\n",
        "  y_test_list.append(y_test.to_numpy())\n",
        "  X_test_list.append(X_test.to_numpy())\n",
        "\n",
        "  i += 1\n",
        "i = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVSD81CqPdjF"
      },
      "outputs": [],
      "source": [
        "meta_features_table = pd.read_csv(meta_table_path)\n",
        "\n",
        "if os.path.isfile('./metafeatures/meta-table.csv'):\n",
        "    meta_table = pd.read_csv('./metafeatures/meta-table.csv')\n",
        "else:\n",
        "    meta_table_columns = meta_features_table.columns.tolist()\n",
        "    for counter in counters:\n",
        "        meta_table_columns.append('arr_' + counter)\n",
        "    meta_table = pd.DataFrame(columns=meta_table_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # meta_table\n",
        "\n",
        "# meta_table.loc[len(meta_table.index)] = meta_features_table.iloc[0].tolist()\n",
        "# meta_table.loc[len(meta_table.index)] = meta_features_table.iloc[1].tolist()\n",
        "# meta_table.loc[len(meta_table.index)] = meta_features_table.iloc[2].tolist()\n",
        "# meta_table.loc[len(meta_table.index)] = meta_features_table.iloc[3].tolist()\n",
        "# meta_table.loc[len(meta_table.index)] = meta_features_table.iloc[4].tolist()\n",
        "\n",
        "# meta_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# meta_table_columns = meta_features_table.columns.tolist()\n",
        "# for counter in counters:\n",
        "#     meta_table_columns.append('arr_' + counter)\n",
        "# meta_table_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMQQS-h31g3c"
      },
      "source": [
        "# meta-features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5laWnKwkJh_X"
      },
      "outputs": [],
      "source": [
        "def run_experiment(X_train, y_train, X_test, y_test, dataset_name):\n",
        "  #......................input/output path directories....................\n",
        "\n",
        "  # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "  \n",
        "  clf = None\n",
        "  # clf = LogisticRegression(random_state=42, n_jobs=-1)\n",
        "  # clf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "  \n",
        "  calib_clf = CalibratedClassifierCV(clf, cv=3, n_jobs=-1)\n",
        "  calib_clf.fit(X_train, y_train)\n",
        "\n",
        "  try:\n",
        "    clf = joblib.load('./estimator_parameters/' + dataset_name + '.joblib')\n",
        "    clf.n_jobs = -1\n",
        "  except:\n",
        "    clf = LogisticRegression(random_state=42, n_jobs=-1)\n",
        "\n",
        "  scores = getTrainingScores(X_train, y_train, 10, clf)[0] # None\n",
        "  tprfpr = getTPRFPR(scores)\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "  # disp = CalibrationDisplay.from_estimator(rf_clf, X_test, y_test)\n",
        "  # disp.plot()\n",
        "\n",
        "  # disp = CalibrationDisplay.from_estimator(calib_clf, X_test, y_test)\n",
        "  # disp.plot()\n",
        "\n",
        "  # plt.show()\n",
        "\n",
        "  niterations = 10 # how many replicates it will take\n",
        "  batch_sizes = list([100]) # list(range(10, min(91, max_allowed + 1), 10))# + list(range(100, min(501, max_allowed + 1), 100)) # test set sizes\n",
        "  alpha_values = [round(x, 2) for x in np.linspace(0,1,20)]   # class proportion\n",
        "  # alpha_values = [x for x in alpha_values if x < 0.4 or x > 0.6]  # removing alpha > 0.4 and alpha < 0.6\n",
        "\n",
        "  pos_scores = scores[scores[\"class\"]==1][\"scores\"]\n",
        "  neg_scores = scores[scores[\"class\"]==0][\"scores\"]\n",
        "  \n",
        "  X_test = pd.DataFrame(X_test)\n",
        "  y_test = pd.DataFrame(y_test, columns=[str(len(X_test.columns))])\n",
        "  df_test = pd.concat([X_test, y_test], axis=1)\n",
        "  \n",
        "  # WAS ZERO (0) BEFORE\n",
        "  df_test_pos = df_test.loc[df_test[df_test.columns[-1]] == 1] # seperating positive test examples\n",
        "  df_test_neg = df_test.loc[df_test[df_test.columns[-1]] == 0] # seperating negative test examples\n",
        "  \n",
        "  # pdb.set_trace()\n",
        "\n",
        "  table=pd.DataFrame(columns=['quantifier', 'abs-error', 'execution-time'])\n",
        "  for sample_size in batch_sizes:   # [10,100,500], batch_sizes, Varying test set sizes\n",
        "\n",
        "    for alpha in alpha_values: # Varying positive class distribution\n",
        "      # abs_error_cc = []\n",
        "      # abs_error_dys = []\n",
        "\n",
        "      error = []\n",
        "\n",
        "      for iter in range(niterations):\n",
        "        pos_size = int(round(sample_size * alpha, 2))\n",
        "        neg_size = sample_size - pos_size\n",
        "\n",
        "        #\n",
        "        # AVISAR O PROF ANDRE QUE SÃ“ FUNCIONOU COM REPLACE = TRUE\n",
        "        #\n",
        "        # df_test_neg\n",
        "        # sample_test_pos = df_test_pos.sample( int(pos_size), replace = False)\n",
        "        # sample_test_neg = df_test_neg.sample( int(neg_size), replace = False)\n",
        "        sample_test_pos = df_test_pos.sample( int(pos_size), replace = True)\n",
        "        sample_test_neg = df_test_neg.sample( int(neg_size), replace = True)\n",
        "\n",
        "        sample_test = pd.concat([sample_test_pos, sample_test_neg])\n",
        "\n",
        "        test_label = sample_test[sample_test.columns[-1]] # sample_test[\"class\"]\n",
        "\n",
        "        test_sample = sample_test.drop([sample_test.columns[-1]], axis=1) # sample_test.drop([\"class\"], axis=1)  #dropping class label columns\n",
        "        te_scores = clf.predict_proba(test_sample)[:,1]  #estimating test sample scores\n",
        "\n",
        "        n_pos_sample_test = list(test_label).count(1) #Counting num of actual positives in test sample\n",
        "        calcultd_pos_prop = round(n_pos_sample_test/len(sample_test), 2) #actual pos class prevalence in generated sample\n",
        "\n",
        "        # print(counters)\n",
        "        for quantifier in counters:\n",
        "          #..............Test Sample QUAPY exp...........................\n",
        "          te_quapy = None\n",
        "          external_qnt = None\n",
        "          #if quantifier in ['EM', 'PWK']:\n",
        "          #  print('ok')\n",
        "          #  external_qnt = fitQuantifierSchumacherGithub(quantifier, X_train, y_train)                  \n",
        "          #  te_quapy = qp.data.LabelledCollection(sample_test.drop([\"class\",\"Binary_label\"], axis=1), test_label)\n",
        "          \n",
        "          #.............Calling of Methods..................................................\n",
        "          start = time.time()\n",
        "          pred_pos_prop = apply_quantifier(qntMethod=quantifier,\n",
        "                                           clf=calib_clf,\n",
        "                                           scores=scores,\n",
        "                                           p_score=pos_scores,\n",
        "                                           n_score=neg_scores,\n",
        "                                           train_labels=None,\n",
        "                                           test_score=te_scores,\n",
        "                                           TprFpr=tprfpr,\n",
        "                                           thr=0.5,\n",
        "                                           measure='hellinger',\n",
        "                                           test_data=test_sample,\n",
        "                                           test_quapy=te_quapy,\n",
        "                                           external_qnt=external_qnt) #y_test=test_label\n",
        "          stop = time.time()\n",
        "          t = stop - start\n",
        "\n",
        "          pred_pos_prop = np.round(pred_pos_prop,2)  #predicted class proportion\n",
        "          \n",
        "          #..............................RESULTS Evaluation.....................................\n",
        "          abs_error = round(abs(calcultd_pos_prop - pred_pos_prop), 2) # absolute error\n",
        "          \n",
        "          table.loc[len(table)] = [quantifier,abs_error,t]\n",
        "          # error = round(calcultd_pos_prop - pred_pos_prop, 2)     # simple error Biasness\n",
        "          \n",
        "  return table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2PwBTyQNRbn",
        "outputId": "08d1f61b-efcd-43dc-eede-44ff7e6d0c28"
      },
      "outputs": [],
      "source": [
        "skip_count = 0\n",
        "table = None\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "result = {}\n",
        "for counter in counters:\n",
        "    result[counter] = []\n",
        "\n",
        "file = open('log.txt', 'w')\n",
        "file.close()\n",
        "\n",
        "result_index = 0\n",
        "for i in range(len(meta_table), len(X_train_list)):\n",
        "  try:\n",
        "    table = run_experiment(X_train_list[i], y_train_list[i], X_test_list[i], y_test_list[i], str(files[i].split('.csv')[0]))\n",
        "\n",
        "    table = table.groupby('quantifier')['abs-error', 'execution-time'].aggregate('mean')\n",
        "\n",
        "    alpha = 0\n",
        "    for key in counters:\n",
        "      try:\n",
        "        sum = 0\n",
        "        for k in counters:\n",
        "          if k != key:\n",
        "            sum += ((1 - table['abs-error'][key]) / (1 - table['abs-error'][k]))\n",
        "            # sum += ((1 - table['abs-error'][key]) / (1 - table['abs-error'][k])) / (1 + alpha * math.log((table['execution-time'][key] / table['execution-time'][k])))\n",
        "        arr = sum / (len(counters) - 1)\n",
        "        result[key].append(arr)\n",
        "      except:\n",
        "        result[key].append(-1)    \n",
        "\n",
        "    row = meta_features_table.iloc[i].tolist()\n",
        "    for key in result:\n",
        "      row.append(result[key][result_index])\n",
        "    meta_table.loc[len(meta_table.index)] = row\n",
        "    \n",
        "    meta_table.to_csv('./metafeatures/meta-table.csv', index = False)\n",
        "\n",
        "    result_index += 1\n",
        "    print('Finished ' + str(i))\n",
        "  except Exception as e:\n",
        "    print('Skipping ' + str(i) + '...\\t\\t\\t' + str(e))\n",
        "    # for key in meta_table_dict:\n",
        "    #   meta_table_dict[key].drop(i, inplace = True)\n",
        "    skip_count += 1\n",
        "    \n",
        "    file = open('log.txt', 'a')\n",
        "    file.write('Skipping ' + str(i) + '...\\t\\t\\t' + str(e) + '\\n')\n",
        "    file.write('Dataset: ' + str(files[i]) + '\\n')\n",
        "    file.write('table[abs-error][key]:\\n' + str(table['abs-error'][key]) + '\\n')\n",
        "    file.write('table[abs-error][k]:\\n' + str(table['abs-error'][k]) + '\\n')\n",
        "    file.write('table[execution-time][key]:\\n' + str(table['execution-time'][key]) + '\\n')\n",
        "    file.write('table[execution-time][k]:\\n' + str(table['execution-time'][k]) + '\\n')\n",
        "    file.write(str(table['execution-time'][key] / table['execution-time'][k]) + '\\n')\n",
        "    file.write('\\n')\n",
        "    file.close()\n",
        "\n",
        "\n",
        "  # # TEST\n",
        "  # if i == 6:\n",
        "  #   break\n",
        "\n",
        "# for key in meta_table_dict:\n",
        "#   meta_table_dict[key]['arr'] = result[key]\n",
        "\n",
        "# for key in result:\n",
        "#     meta_features_table[('arr_' + key)] = result[key]\n",
        "\n",
        "print('\\n\\nSkipped ' + str(skip_count) + ' dataset(s)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# meta_features_table.to_csv('./metafeatures/meta-table.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pickle\n",
        "\n",
        "# with open('result.pkl', 'wb') as fp:\n",
        "#     pickle.dump(result, fp)\n",
        "#     print('dictionary saved successfully to file')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with open('result.pkl', 'rb') as fp:\n",
        "#     person = pickle.load(fp)\n",
        "#     print(person)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# result\n",
        "# meta_table_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# finSavePath = './metafeatures/meta-table'\n",
        "# for c in counters:\n",
        "#     meta_table_dict[c].to_csv(str(finSavePath + '-' + c + '.csv'), index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# test = pd.read_csv('./metafeatures/meta-table-PACC.csv')\n",
        "# test = test.pop(test.columns[-1])\n",
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "\n",
        "# i = 0\n",
        "# for t in test:\n",
        "#     if np.isnan(t):\n",
        "#         print(i)\n",
        "#     i += 1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
